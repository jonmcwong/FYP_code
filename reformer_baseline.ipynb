{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# code based off of \n",
    "# https://github.com/mandubian/pytorch_math_dataset and\n",
    "# https://github.com/lucidrains/reformer-pytorch\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "from apex import amp\n",
    "import pickle\n",
    "\n",
    "\n",
    "import mandubian.math_dataset\n",
    "from mandubian.math_dataset import MathDatasetManager\n",
    "from mandubian.transformer import Constants\n",
    "\n",
    "# from transformer.Models import Transformer\n",
    "from mandubian.math_dataset import (\n",
    "    random_split_dataset,\n",
    "    question_answer_to_mask_batch_collate_fn\n",
    ")\n",
    "from mandubian.math_dataset import np_encode_string, np_decode_string\n",
    "import mandubian.model_process\n",
    "import mandubian.utils\n",
    "from mandubian.tensorboard_utils import Tensorboard\n",
    "from mandubian.tensorboard_utils import tensorboard_event_accumulator\n",
    "\n",
    "import mandubian.checkpoints\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 detected CUDA devices\n",
      "Using CUDA device:  0\n",
      "GeForce RTX 2080\n",
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "print(torch.cuda.device_count(), \"detected CUDA devices\")\n",
    "cuda_device = torch.cuda.current_device()\n",
    "print(\"Using CUDA device: \", cuda_device)\n",
    "print(torch.cuda.get_device_name(cuda_device))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucidrains_reformer.reformer_pytorch import ReformerLM, Autopadder, Recorder\n",
    "from lucidrains_reformer.reformer_pytorch import ReformerEncDec\n",
    "from lucidrains_reformer.reformer_pytorch.generative_tools import TrainingWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Math Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized MultiFilesMathDataset with categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus'] and types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "mdsmgr structure ['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_datasets_from_category', 'build_dataset_from_categories', 'build_dataset_from_category', 'build_dataset_from_module', 'build_dataset_from_modules', 'dfs', 'dirs', 'get_categories', 'get_modules_for_category', 'get_types', 'root_dir']\n"
     ]
    }
   ],
   "source": [
    "mdsmgr = MathDatasetManager(\n",
    "  \"/home/jonathan/Repos/final_year_at_ic/awesome_project/mathematics_dataset-v1.0/\"\n",
    ")\n",
    "# Examine dataset structure\n",
    "print(\"mdsmgr structure\", dir(mdsmgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MathDatasetManager._build_datasets_from_category of <mandubian.math_dataset.MathDatasetManager object at 0x7efb71342c18>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(MathDatasetManager.__dir__\n",
    "mdsmgr._build_datasets_from_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availables types, problem categories and problem subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus']\n",
      "modules of arithmetic dict_keys(['div', 'nearest_integer_root', 'mul_div_multiple', 'mul', 'add_or_sub', 'add_sub_multiple', 'mixed', 'add_or_sub_in_base', 'simplify_surd', 'add_or_sub_big', 'add_sub_multiple_longer', 'mixed_longer', 'div_big', 'mul_div_multiple_longer', 'mul_big'])\n"
     ]
    }
   ],
   "source": [
    "print(\"types\", list(mdsmgr.get_types()))\n",
    "print(\"categories\", list(mdsmgr.get_categories()))\n",
    "print(\"modules of arithmetic\", mdsmgr.get_modules_for_category('arithmetic'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to manipulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# # Build Dataset from a single module in a category\n",
    "ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy')\n",
    "print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from a single module in a category with limited number of elements\n",
    "# ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy', max_elements=1000)\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from several modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'train-easy')\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'interpolate')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in several categories\n",
    "# ds = mdsmgr.build_dataset_from_categories(['arithmetic', 'polynomials'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"baseline_64\"\n",
    "now = datetime.now()\n",
    "unique_id = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "base_dir = \"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tests/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mandubian.math_dataset import (\n",
    "    VOCAB_SZ, MAX_QUESTION_SZ, MAX_ANSWER_SZ\n",
    ")\n",
    "\n",
    "NUM_CPU_THREADS = 12\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATE_EVERY = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 20\n",
    "GENERATE_EVERY  = 60\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# hyperparameters need updates\n",
    "\n",
    "Q_SEQ_LEN = 256\n",
    "A_SEQ_LEN = 30 # unused due to requirements of axial_positon_shape\n",
    "NUM_TOKENS = VOCAB_SZ + 1\n",
    "D_MODEL = 512\n",
    "EMB_DIM = D_MODEL\n",
    "NUM_HEADS = 8\n",
    "QKV_DIM = D_MODEL / NUM_HEADS\n",
    "NUM_LAYERS = 6\n",
    "D_FF = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "# training_data = mdsmgr.build_dataset_from_category('arithmetic','train-easy') # for now\n",
    "training_data = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy')\n",
    "\n",
    "# testing data\n",
    "# testing_data_interpolate = mdsmgr.build_dataset_from_category('arithmetic','interpolate')\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_category('arithmetic','extrapolate')\n",
    "\n",
    "testing_data_interpolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'interpolate', max_elements = 1024)\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'extrapolate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lucidrains_reformer.examples.enwik8_simple.train\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "def get_non_pad_mask(seq):\n",
    "    # returns true when token is not PAD and false otherwise\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(Constants.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "# get data splits\n",
    "train_ds, val_ds = mandubian.math_dataset.random_split_dataset(training_data,split_rate=0.9)\n",
    "\n",
    "# get pytorch dataloaders\n",
    "# Questions are padded in question_answer_to_position_batch_collate_fn\n",
    "train_loader = data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "train_loader = cycle(train_loader)\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "val_loader = cycle(val_loader)\n",
    "\n",
    "# for viewing output sequences\n",
    "gen_loader = data.DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "gen_loader = cycle(gen_loader)\n",
    "\n",
    "interpolate_loader = data.DataLoader(\n",
    "    testing_data_interpolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn, pin_memory = True)\n",
    "interpolate_loader = cycle(interpolate_loader)\n",
    "\n",
    "# extrapolate_loader = data.DataLoader(\n",
    "#     testing_data_extrapolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "#     collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "# extrapolate_loader = cycle(extrapolate_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncDec(\n",
       "  (enc): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x4x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x64x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x2x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): Linear(in_features=512, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "enc_dec = ReformerEncDec(\n",
    "    dim = D_MODEL,\n",
    "    enc_num_tokens = NUM_TOKENS,\n",
    "    enc_depth = NUM_LAYERS,\n",
    "    enc_max_seq_len = Q_SEQ_LEN,\n",
    "    dec_num_tokens = NUM_TOKENS,\n",
    "    dec_depth = NUM_LAYERS,\n",
    "    dec_max_seq_len = Q_SEQ_LEN,\n",
    "    # heads = 8 by default\n",
    "    axial_position_shape = (16, 16),  # the shape must multiply up to the max_seq_len (128 x 64 = 8192)\n",
    "    axial_position_dims = (256,256),   # the dims must sum up to the model dimensions (512 + 512 = 1024)\n",
    "    pad_value = Constants.PAD,\n",
    "    ignore_index = Constants.PAD # see if this works. pad_value and ignore_index are probably different\n",
    ").cuda()\n",
    "\n",
    "# enc_dec = Recorder(enc_dec)\n",
    "enc_dec.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer learning rate scheduler, mixed precision setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(enc_dec.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.995), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=100, verbose=True)\n",
    "\n",
    "# mixed precision\n",
    "enc_dec, optimizer = amp.initialize(enc_dec, optimizer, opt_level='O2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "i = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 \t training loss: 1.2412109375 \t 20:09:50.942948\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "validation loss: 5.04296875\n",
      "**************************************************************************************************** \n",
      "Question:  !Evaluate 13 + (4 + -16 - 9).\"                                                                                                                                                                                                                                  \n",
      "Actual Answer:  !-8\"                                                                                                                                                                                                                                                            \n",
      "Decoded Prediction:  ^{AkeW&$0c~ {^)K^) #8U&,twPAj\\Aj\n",
      "Step  1 \t training loss: 1.2724609375 \t 20:09:57.028725\n",
      "Step  2 \t training loss: 1.0400390625 \t 20:09:59.912472\n",
      "Step  3 \t training loss: 0.86328125 \t 20:10:02.792618\n",
      "Step  4 \t training loss: 0.72607421875 \t 20:10:05.578192\n",
      "Step  5 \t training loss: 0.671875 \t 20:10:08.368965\n",
      "Step  6 \t training loss: 0.61767578125 \t 20:10:11.147066\n",
      "Step  7 \t training loss: 0.6123046875 \t 20:10:13.926787\n",
      "Step  8 \t training loss: 0.5859375 \t 20:10:16.718494\n",
      "Step  9 \t training loss: 0.55419921875 \t 20:10:19.489283\n",
      "Step  10 \t training loss: 0.5703125 \t 20:10:22.282031\n",
      "Step  11 \t training loss: 0.576171875 \t 20:10:25.076627\n",
      "Step  12 \t training loss: 0.54833984375 \t 20:10:27.932042\n",
      "Step  13 \t training loss: 0.568359375 \t 20:10:30.804662\n",
      "Step  14 \t training loss: 0.60986328125 \t 20:10:33.693340\n",
      "Step  15 \t training loss: 0.51318359375 \t 20:10:36.575080\n",
      "Step  16 \t training loss: 0.51953125 \t 20:10:39.357335\n",
      "Step  17 \t training loss: 0.53271484375 \t 20:10:42.204520\n",
      "Step  18 \t training loss: 0.5322265625 \t 20:10:44.977122\n",
      "Step  19 \t training loss: 0.4990234375 \t 20:10:47.752365\n",
      "Step  20 \t training loss: 0.5107421875 \t 20:10:50.535054\n",
      "validation loss: 2.052734375\n",
      "Step  21 \t training loss: 0.54638671875 \t 20:10:53.684085\n",
      "Step  22 \t training loss: 0.525390625 \t 20:10:56.465469\n",
      "Step  23 \t training loss: 0.4951171875 \t 20:10:59.255246\n",
      "Step  24 \t training loss: 0.494140625 \t 20:11:02.037089\n",
      "Step  25 \t training loss: 0.46728515625 \t 20:11:04.828937\n",
      "Step  26 \t training loss: 0.52783203125 \t 20:11:07.611307\n",
      "Step  27 \t training loss: 0.48193359375 \t 20:11:10.394124\n",
      "Step  28 \t training loss: 0.499267578125 \t 20:11:13.200465\n",
      "Step  29 \t training loss: 0.464599609375 \t 20:11:16.089744\n",
      "Step  30 \t training loss: 0.47900390625 \t 20:11:18.983317\n",
      "Step  31 \t training loss: 0.51904296875 \t 20:11:21.862940\n",
      "Step  32 \t training loss: 0.47314453125 \t 20:11:24.663069\n",
      "Step  33 \t training loss: 0.51171875 \t 20:11:27.445110\n",
      "Step  34 \t training loss: 0.49951171875 \t 20:11:30.240103\n",
      "Step  35 \t training loss: 0.48046875 \t 20:11:33.038945\n",
      "Step  36 \t training loss: 0.495361328125 \t 20:11:35.823359\n",
      "Step  37 \t training loss: 0.50537109375 \t 20:11:38.614856\n",
      "Step  38 \t training loss: 0.49951171875 \t 20:11:41.400650\n",
      "Step  39 \t training loss: 0.484375 \t 20:11:44.216136\n",
      "Step  40 \t training loss: 0.47119140625 \t 20:11:47.063015\n",
      "validation loss: 1.814453125\n",
      "Step  41 \t training loss: 0.51025390625 \t 20:11:50.327565\n",
      "Step  42 \t training loss: 0.52197265625 \t 20:11:53.209260\n",
      "Step  43 \t training loss: 0.476806640625 \t 20:11:56.016769\n",
      "Step  44 \t training loss: 0.47021484375 \t 20:11:58.836013\n",
      "Step  45 \t training loss: 0.465087890625 \t 20:12:01.638650\n",
      "Step  46 \t training loss: 0.4765625 \t 20:12:04.454298\n",
      "Step  47 \t training loss: 0.477783203125 \t 20:12:07.265724\n",
      "Step  48 \t training loss: 0.4482421875 \t 20:12:10.078186\n",
      "Step  49 \t training loss: 0.482177734375 \t 20:12:12.890452\n",
      "Step  50 \t training loss: 0.5126953125 \t 20:12:15.709971\n",
      "Step  51 \t training loss: 0.468017578125 \t 20:12:18.515784\n",
      "Step  52 \t training loss: 0.493896484375 \t 20:12:21.311601\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    # exclude the 0th element as it is BOS\n",
    "    \n",
    "    if (i % GENERATE_EVERY) - 1 == 0:\n",
    "        enc_dec.eval()\n",
    "        gen_qs, gen_qs_mask, gen_as, gen_as_mask = next(gen_loader)\n",
    "        prime = np_decode_string(gen_qs.numpy())\n",
    "        print('*' * 100, \"\\nQuestion: \", prime)\n",
    "        print(\"Actual Answer: \", np_decode_string(gen_as.numpy()))\n",
    "        gen_qs = gen_qs.to(device, non_blocking=True)\n",
    "        gen_as = gen_as.to(device, non_blocking=True)\n",
    "        gen_qs_mask = gen_qs_mask.to(device, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            sample = enc_dec.generate(gen_qs, gen_as[:,0:1], GENERATE_LENGTH, enc_input_mask = gen_qs_mask, dec_eos_token=Constants.EOS)\n",
    "        sample = sample.cpu().numpy()\n",
    "        output_str = np_decode_string(sample)\n",
    "        print(\"Decoded Prediction: \", output_str)\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-train_loss.txt\", train_loss_list)\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\", val_loss_list)\n",
    "            \n",
    "\n",
    "    enc_dec.train()\n",
    "    train_loss_record = 0\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        batch_qs, batch_qs_mask, batch_as, batch_as_mask = map(lambda x: x.to(device, non_blocking=True), next(train_loader))\n",
    "        train_loss = enc_dec(batch_qs, batch_as, return_loss = True, enc_input_mask = batch_qs_mask)\n",
    "        del batch_qs, batch_qs_mask, batch_as, batch_as_mask\n",
    "        with amp.scale_loss(train_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        train_loss_record += float(train_loss)\n",
    "        del train_loss\n",
    "\n",
    "#     if i % GRADIENT_ACCUMULATE_EVERY == 0:\n",
    "    train_loss_record /= 4\n",
    "    print(\"Step \", i, \"\\t\", f'training loss: {train_loss_record}', \"\\t\", datetime.now().time() )\n",
    "    train_loss_list.append((i, train_loss_record))\n",
    "    torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step(train_loss_record)\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        enc_dec.eval()\n",
    "        val_batch_qs, val_batch_qs_mask, val_batch_as, val_batch_as_mask = map(lambda x: x.to(device, non_blocking=True), next(val_loader))\n",
    "        with torch.no_grad():\n",
    "            val_loss = enc_dec(val_batch_qs, val_batch_as, return_loss = True, enc_input_mask = val_batch_qs_mask)\n",
    "            print(f'validation loss: {val_loss.item()}')\n",
    "            val_loss_list.append((i, val_loss.item()))\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([train_loss_list[1]])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
