{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# code based off of \n",
    "# https://github.com/mandubian/pytorch_math_dataset and\n",
    "# https://github.com/lucidrains/reformer-pytorch\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "from apex import amp\n",
    "import pickle\n",
    "\n",
    "\n",
    "import mandubian.math_dataset\n",
    "from mandubian.math_dataset import MathDatasetManager\n",
    "from mandubian.transformer import Constants\n",
    "\n",
    "# from transformer.Models import Transformer\n",
    "from mandubian.math_dataset import (\n",
    "    random_split_dataset,\n",
    "    question_answer_to_mask_batch_collate_fn\n",
    ")\n",
    "from mandubian.math_dataset import np_encode_string, np_decode_string\n",
    "import mandubian.model_process\n",
    "import mandubian.utils\n",
    "from mandubian.tensorboard_utils import Tensorboard\n",
    "from mandubian.tensorboard_utils import tensorboard_event_accumulator\n",
    "\n",
    "import mandubian.checkpoints\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 detected CUDA devices\n",
      "Using CUDA device:  0\n",
      "GeForce RTX 2080\n",
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "print(torch.cuda.device_count(), \"detected CUDA devices\")\n",
    "cuda_device = torch.cuda.current_device()\n",
    "print(\"Using CUDA device: \", cuda_device)\n",
    "print(torch.cuda.get_device_name(cuda_device))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucidrains_reformer.reformer_pytorch import ReformerLM, Autopadder, Recorder\n",
    "from lucidrains_reformer.reformer_pytorch import ReformerEncDec\n",
    "from lucidrains_reformer.reformer_pytorch.generative_tools import TrainingWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Math Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized MultiFilesMathDataset with categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus'] and types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "mdsmgr structure ['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_datasets_from_category', 'build_dataset_from_categories', 'build_dataset_from_category', 'build_dataset_from_module', 'build_dataset_from_modules', 'dfs', 'dirs', 'get_categories', 'get_modules_for_category', 'get_types', 'root_dir']\n"
     ]
    }
   ],
   "source": [
    "mdsmgr = MathDatasetManager(\n",
    "  \"/home/jonathan/Repos/final_year_at_ic/awesome_project/mathematics_dataset-v1.0/\"\n",
    ")\n",
    "# Examine dataset structure\n",
    "print(\"mdsmgr structure\", dir(mdsmgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MathDatasetManager._build_datasets_from_category of <mandubian.math_dataset.MathDatasetManager object at 0x7efb71344cc0>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(MathDatasetManager.__dir__\n",
    "mdsmgr._build_datasets_from_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availables types, problem categories and problem subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus']\n",
      "modules of arithmetic dict_keys(['div', 'nearest_integer_root', 'mul_div_multiple', 'mul', 'add_or_sub', 'add_sub_multiple', 'mixed', 'add_or_sub_in_base', 'simplify_surd', 'add_or_sub_big', 'add_sub_multiple_longer', 'mixed_longer', 'div_big', 'mul_div_multiple_longer', 'mul_big'])\n"
     ]
    }
   ],
   "source": [
    "print(\"types\", list(mdsmgr.get_types()))\n",
    "print(\"categories\", list(mdsmgr.get_categories()))\n",
    "print(\"modules of arithmetic\", mdsmgr.get_modules_for_category('arithmetic'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to manipulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# # Build Dataset from a single module in a category\n",
    "ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy')\n",
    "print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from a single module in a category with limited number of elements\n",
    "# ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy', max_elements=1000)\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from several modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'train-easy')\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'interpolate')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in several categories\n",
    "# ds = mdsmgr.build_dataset_from_categories(['arithmetic', 'polynomials'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"overfit_one_batch_142_V2\"\n",
    "now = datetime.now()\n",
    "unique_id = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "base_dir = \"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tests/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mandubian.math_dataset import (\n",
    "    VOCAB_SZ, MAX_QUESTION_SZ, MAX_ANSWER_SZ\n",
    ")\n",
    "\n",
    "NUM_CPU_THREADS = 12\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 20\n",
    "GENERATE_EVERY  = 60\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# hyperparameters need updates\n",
    "\n",
    "Q_SEQ_LEN = 256\n",
    "A_SEQ_LEN = 30 # unused due to requirements of axial_positon_shape\n",
    "NUM_TOKENS = VOCAB_SZ + 1\n",
    "D_MODEL = 512\n",
    "EMB_DIM = D_MODEL\n",
    "NUM_HEADS = 8\n",
    "QKV_DIM = D_MODEL / NUM_HEADS\n",
    "NUM_LAYERS = 6\n",
    "D_FF = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "# training_data = mdsmgr.build_dataset_from_category('arithmetic','train-easy') # for now\n",
    "training_data = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy', max_elements = 142)\n",
    "\n",
    "# testing data\n",
    "# testing_data_interpolate = mdsmgr.build_dataset_from_category('arithmetic','interpolate')\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_category('arithmetic','extrapolate')\n",
    "\n",
    "testing_data_interpolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'interpolate', max_elements = 1024)\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'extrapolate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lucidrains_reformer.examples.enwik8_simple.train\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "def get_non_pad_mask(seq):\n",
    "    # returns true when token is not PAD and false otherwise\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(Constants.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "# get data splits\n",
    "train_ds, val_ds = mandubian.math_dataset.random_split_dataset(training_data,split_rate=0.9)\n",
    "\n",
    "# get pytorch dataloaders\n",
    "# Questions are padded in question_answer_to_position_batch_collate_fn\n",
    "train_loader = data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "train_loader = cycle(train_loader)\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "val_loader = cycle(val_loader)\n",
    "\n",
    "# for viewing output sequences\n",
    "gen_loader = data.DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "gen_loader = cycle(gen_loader)\n",
    "\n",
    "interpolate_loader = data.DataLoader(\n",
    "    testing_data_interpolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "interpolate_loader = cycle(interpolate_loader)\n",
    "\n",
    "# extrapolate_loader = data.DataLoader(\n",
    "#     testing_data_extrapolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "#     collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "# extrapolate_loader = cycle(extrapolate_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncDec(\n",
       "  (enc): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x4x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x64x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x2x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): Linear(in_features=512, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "enc_dec = ReformerEncDec(\n",
    "    dim = D_MODEL,\n",
    "    enc_num_tokens = NUM_TOKENS,\n",
    "    enc_depth = NUM_LAYERS,\n",
    "    enc_max_seq_len = Q_SEQ_LEN,\n",
    "    dec_num_tokens = NUM_TOKENS,\n",
    "    dec_depth = NUM_LAYERS,\n",
    "    dec_max_seq_len = Q_SEQ_LEN,\n",
    "    # heads = 8 by default\n",
    "    axial_position_shape = (64, 16),  # the shape must multiply up to the max_seq_len (128 x 64 = 8192)\n",
    "    axial_position_dims = (256,256),   # the dims must sum up to the model dimensions (512 + 512 = 1024)\n",
    "    pad_value = Constants.PAD,\n",
    "    ignore_index = Constants.PAD # see if this works. pad_value and ignore_index are probably different\n",
    ").cuda()\n",
    "\n",
    "# enc_dec = Recorder(enc_dec)\n",
    "enc_dec.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer learning rate scheduler, mixed precision setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(enc_dec.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.995), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=100, verbose=True)\n",
    "\n",
    "# mixed precision\n",
    "enc_dec, optimizer = amp.initialize(enc_dec, optimizer, opt_level='O2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 \t training loss: 4.875 \t 12:41:58.103986\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "validation loss: 4.96484375\n",
      "**************************************************************************************************** \n",
      "Question:  !What is the value of (10 - 5) + (-6 - -3 - 1)?\"                                                                                                                                                                                                                \n",
      "Actual Answer:  !1\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  111.!.pNS<g 3+ 3...!N.TLp.n ..n.\n",
      "Step  1 \t training loss: 4.9296875 \t 12:42:12.328322\n",
      "Step  2 \t training loss: 3.716796875 \t 12:42:23.548709\n",
      "Step  3 \t training loss: 3.33203125 \t 12:42:34.746052\n",
      "Step  4 \t training loss: 2.900390625 \t 12:42:45.957931\n",
      "Step  5 \t training loss: 2.712890625 \t 12:42:57.163718\n",
      "Step  6 \t training loss: 2.556640625 \t 12:43:08.435662\n",
      "Step  7 \t training loss: 2.5234375 \t 12:43:20.246330\n",
      "Step  8 \t training loss: 2.36328125 \t 12:43:31.922943\n",
      "Step  9 \t training loss: 2.318359375 \t 12:43:43.189785\n",
      "Step  10 \t training loss: 2.279296875 \t 12:43:54.357135\n",
      "Step  11 \t training loss: 2.044921875 \t 12:44:05.542449\n",
      "Step  12 \t training loss: 2.341796875 \t 12:44:16.732879\n",
      "Step  13 \t training loss: 2.279296875 \t 12:44:27.986210\n",
      "Step  14 \t training loss: 2.16015625 \t 12:44:39.228000\n",
      "Step  15 \t training loss: 2.095703125 \t 12:44:50.806899\n",
      "Step  16 \t training loss: 2.07421875 \t 12:45:02.302998\n",
      "Step  17 \t training loss: 2.08984375 \t 12:45:13.538897\n",
      "Step  18 \t training loss: 2.25390625 \t 12:45:24.774214\n",
      "Step  19 \t training loss: 2.01953125 \t 12:45:36.018953\n",
      "Step  20 \t training loss: 2.0234375 \t 12:45:47.248379\n",
      "validation loss: 2.15625\n",
      "Step  21 \t training loss: 2.01171875 \t 12:45:59.073411\n",
      "Step  22 \t training loss: 2.0078125 \t 12:46:10.313141\n",
      "Step  23 \t training loss: 2.10546875 \t 12:46:21.890947\n",
      "Step  24 \t training loss: 2.05859375 \t 12:46:33.380593\n",
      "Step  25 \t training loss: 2.068359375 \t 12:46:44.595437\n",
      "Step  26 \t training loss: 2.111328125 \t 12:46:55.845208\n",
      "Step  27 \t training loss: 1.869140625 \t 12:47:07.087620\n",
      "Step  28 \t training loss: 1.8994140625 \t 12:47:18.329907\n",
      "Step  29 \t training loss: 1.90234375 \t 12:47:29.574298\n",
      "Step  30 \t training loss: 1.7861328125 \t 12:47:40.801701\n",
      "Step  31 \t training loss: 1.849609375 \t 12:47:52.395518\n",
      "Step  32 \t training loss: 1.8779296875 \t 12:48:03.882775\n",
      "Step  33 \t training loss: 1.935546875 \t 12:48:15.115466\n",
      "Step  34 \t training loss: 1.8427734375 \t 12:48:26.369183\n",
      "Step  35 \t training loss: 1.8779296875 \t 12:48:37.602412\n",
      "Step  36 \t training loss: 1.9013671875 \t 12:48:48.838788\n",
      "Step  37 \t training loss: 1.76953125 \t 12:49:00.089574\n",
      "Step  38 \t training loss: 1.83203125 \t 12:49:11.318928\n",
      "Step  39 \t training loss: 1.7919921875 \t 12:49:22.896407\n",
      "Step  40 \t training loss: 1.677734375 \t 12:49:34.351372\n",
      "validation loss: 1.98828125\n",
      "Step  41 \t training loss: 1.8173828125 \t 12:49:46.180888\n",
      "Step  42 \t training loss: 1.8779296875 \t 12:49:57.403239\n",
      "Step  43 \t training loss: 1.8564453125 \t 12:50:08.668039\n",
      "Step  44 \t training loss: 1.8701171875 \t 12:50:19.960850\n",
      "Step  45 \t training loss: 1.8349609375 \t 12:50:31.219637\n",
      "Step  46 \t training loss: 1.9072265625 \t 12:50:42.490956\n",
      "Step  47 \t training loss: 1.7373046875 \t 12:50:54.135582\n",
      "Step  48 \t training loss: 1.7734375 \t 12:51:05.642288\n",
      "Step  49 \t training loss: 1.8701171875 \t 12:51:16.917552\n",
      "Step  50 \t training loss: 1.810546875 \t 12:51:28.184841\n",
      "Step  51 \t training loss: 1.8095703125 \t 12:51:39.446412\n",
      "Step  52 \t training loss: 1.7705078125 \t 12:51:50.714867\n",
      "Step  53 \t training loss: 1.9296875 \t 12:52:01.997275\n",
      "Step  54 \t training loss: 1.7060546875 \t 12:52:13.265002\n",
      "Step  55 \t training loss: 1.736328125 \t 12:52:24.902728\n",
      "Step  56 \t training loss: 1.7919921875 \t 12:52:36.423964\n",
      "Step  57 \t training loss: 1.7451171875 \t 12:52:47.700082\n",
      "Step  58 \t training loss: 1.6474609375 \t 12:52:58.958890\n",
      "Step  59 \t training loss: 1.70703125 \t 12:53:10.234293\n",
      "Step  60 \t training loss: 1.8408203125 \t 12:53:21.507241\n",
      "validation loss: 1.9453125\n",
      "**************************************************************************************************** \n",
      "Question:  !-4 + (6 - 11) + 4\"                                                                                                                                                                                                                                             \n",
      "Actual Answer:  !-5\"                                                                                                                                                                                                                                                            \n",
      "Decoded Prediction:  48.5\"fmTT1\"81\" \"88\"\"\"f 1\"3\" .2H<\n",
      "Step  61 \t training loss: 1.73046875 \t 12:53:35.525030\n",
      "Step  62 \t training loss: 1.791015625 \t 12:53:46.756250\n",
      "Step  63 \t training loss: 1.744140625 \t 12:53:58.327873\n",
      "Step  64 \t training loss: 1.6923828125 \t 12:54:09.849934\n",
      "Step  65 \t training loss: 1.7294921875 \t 12:54:21.078403\n",
      "Step  66 \t training loss: 1.7275390625 \t 12:54:32.313470\n",
      "Step  67 \t training loss: 1.5234375 \t 12:54:43.558126\n",
      "Step  68 \t training loss: 1.646484375 \t 12:54:54.789611\n",
      "Step  69 \t training loss: 1.701171875 \t 12:55:06.024103\n",
      "Step  70 \t training loss: 1.78125 \t 12:55:17.269046\n",
      "Step  71 \t training loss: 1.7314453125 \t 12:55:28.853168\n",
      "Step  72 \t training loss: 1.6630859375 \t 12:55:40.349460\n",
      "Step  73 \t training loss: 1.4443359375 \t 12:55:51.589509\n",
      "Step  74 \t training loss: 1.650390625 \t 12:56:02.862386\n",
      "Step  75 \t training loss: 1.65625 \t 12:56:14.135755\n",
      "Step  76 \t training loss: 1.7470703125 \t 12:56:25.397875\n",
      "Step  77 \t training loss: 1.6669921875 \t 12:56:36.654712\n",
      "Step  78 \t training loss: 1.6640625 \t 12:56:47.935372\n",
      "Step  79 \t training loss: 1.681640625 \t 12:56:59.565979\n",
      "Step  80 \t training loss: 1.6650390625 \t 12:57:11.080692\n",
      "validation loss: 1.9326171875\n",
      "Step  81 \t training loss: 1.478515625 \t 12:57:22.933996\n",
      "Step  82 \t training loss: 1.640625 \t 12:57:34.202222\n",
      "Step  83 \t training loss: 1.5673828125 \t 12:57:45.473309\n",
      "Step  84 \t training loss: 1.7265625 \t 12:57:56.734199\n",
      "Step  85 \t training loss: 1.7275390625 \t 12:58:08.009151\n",
      "Step  86 \t training loss: 1.5732421875 \t 12:58:19.286858\n",
      "Step  87 \t training loss: 1.6630859375 \t 12:58:30.926848\n",
      "Step  88 \t training loss: 1.650390625 \t 12:58:42.465610\n",
      "Step  89 \t training loss: 1.634765625 \t 12:58:53.585688\n",
      "Step  90 \t training loss: 1.552734375 \t 12:59:04.378234\n",
      "Step  91 \t training loss: 1.5244140625 \t 12:59:15.171157\n",
      "Step  92 \t training loss: 1.556640625 \t 12:59:25.964685\n",
      "Step  93 \t training loss: 1.6083984375 \t 12:59:36.753172\n",
      "Step  94 \t training loss: 1.5517578125 \t 12:59:47.545039\n",
      "Step  95 \t training loss: 1.6787109375 \t 12:59:58.666969\n",
      "Step  96 \t training loss: 1.392578125 \t 13:00:09.708172\n",
      "Step  97 \t training loss: 1.6201171875 \t 13:00:20.519144\n",
      "Step  98 \t training loss: 1.619140625 \t 13:00:31.318824\n",
      "Step  99 \t training loss: 1.615234375 \t 13:00:42.124701\n",
      "Step  100 \t training loss: 1.57421875 \t 13:00:52.921380\n",
      "validation loss: 1.9033203125\n",
      "Step  101 \t training loss: 1.5673828125 \t 13:01:04.294173\n",
      "Step  102 \t training loss: 1.5556640625 \t 13:01:15.086647\n",
      "Step  103 \t training loss: 1.5263671875 \t 13:01:26.227008\n",
      "Step  104 \t training loss: 1.46484375 \t 13:01:37.278818\n",
      "Step  105 \t training loss: 1.587890625 \t 13:01:48.069686\n",
      "Step  106 \t training loss: 1.6318359375 \t 13:01:58.861502\n",
      "Step  107 \t training loss: 1.41796875 \t 13:02:09.662145\n",
      "Step  108 \t training loss: 1.6083984375 \t 13:02:20.464873\n",
      "Step  109 \t training loss: 1.5654296875 \t 13:02:31.260459\n",
      "Step  110 \t training loss: 1.5107421875 \t 13:02:42.050892\n",
      "Step  111 \t training loss: 1.5615234375 \t 13:02:53.180680\n",
      "Step  112 \t training loss: 1.37890625 \t 13:03:04.226916\n",
      "Step  113 \t training loss: 1.4990234375 \t 13:03:15.013496\n",
      "Step  114 \t training loss: 1.5546875 \t 13:03:25.806039\n",
      "Step  115 \t training loss: 1.5146484375 \t 13:03:36.590982\n",
      "Step  116 \t training loss: 1.5126953125 \t 13:03:47.385866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  117 \t training loss: 1.60546875 \t 13:03:58.175013\n",
      "Step  118 \t training loss: 1.4912109375 \t 13:04:08.958444\n",
      "Step  119 \t training loss: 1.5341796875 \t 13:04:20.092188\n",
      "Step  120 \t training loss: 1.5244140625 \t 13:04:31.114749\n",
      "validation loss: 1.8994140625\n",
      "**************************************************************************************************** \n",
      "Question:  !Total of -1.7 and -8658.\"                                                                                                                                                                                                                                      \n",
      "Actual Answer:  !-8659.7\"                                                                                                                                                                                                                                                       \n",
      "Decoded Prediction:  .7\"1\" \"\" 3\"8\"\"8\"1\" 8\"\"\"@\"\"1\"\".4\"\n",
      "Step  121 \t training loss: 1.515625 \t 13:04:44.653851\n",
      "Step  122 \t training loss: 1.404296875 \t 13:04:55.439619\n",
      "Step  123 \t training loss: 1.4951171875 \t 13:05:06.219902\n",
      "Step  124 \t training loss: 1.46484375 \t 13:05:16.997332\n",
      "Step  125 \t training loss: 1.4248046875 \t 13:05:27.779539\n",
      "Step  126 \t training loss: 1.4931640625 \t 13:05:38.557472\n",
      "Step  127 \t training loss: 1.4169921875 \t 13:05:49.681913\n",
      "Step  128 \t training loss: 1.4326171875 \t 13:06:00.728632\n",
      "Step  129 \t training loss: 1.478515625 \t 13:06:11.512771\n",
      "Step  130 \t training loss: 1.28125 \t 13:06:22.309098\n",
      "Step  131 \t training loss: 1.5185546875 \t 13:06:33.109701\n",
      "Step  132 \t training loss: 1.4248046875 \t 13:06:43.898982\n",
      "Step  133 \t training loss: 1.3818359375 \t 13:06:54.689347\n",
      "Step  134 \t training loss: 1.48828125 \t 13:07:05.469516\n",
      "Step  135 \t training loss: 1.466796875 \t 13:07:16.591114\n",
      "Step  136 \t training loss: 1.4521484375 \t 13:07:27.621761\n",
      "Step  137 \t training loss: 1.4150390625 \t 13:07:38.419080\n",
      "Step  138 \t training loss: 1.251953125 \t 13:07:49.214428\n",
      "Step  139 \t training loss: 1.244140625 \t 13:08:00.009236\n",
      "Step  140 \t training loss: 1.478515625 \t 13:08:10.805238\n",
      "validation loss: 1.9365234375\n",
      "Step  141 \t training loss: 1.5361328125 \t 13:08:22.179686\n",
      "Step  142 \t training loss: 1.4775390625 \t 13:08:32.969405\n",
      "Step  143 \t training loss: 1.4326171875 \t 13:08:44.095781\n",
      "Step  144 \t training loss: 1.2998046875 \t 13:08:55.133167\n",
      "Step  145 \t training loss: 1.349609375 \t 13:09:05.915639\n",
      "Step  146 \t training loss: 1.3740234375 \t 13:09:16.698135\n",
      "Step  147 \t training loss: 1.287109375 \t 13:09:27.482444\n",
      "Step  148 \t training loss: 1.4287109375 \t 13:09:38.267143\n",
      "Step  149 \t training loss: 1.5439453125 \t 13:09:49.053499\n",
      "Step  150 \t training loss: 1.404296875 \t 13:09:59.832573\n",
      "Step  151 \t training loss: 1.4521484375 \t 13:10:10.951490\n",
      "Step  152 \t training loss: 1.306640625 \t 13:10:22.004333\n",
      "Step  153 \t training loss: 1.341796875 \t 13:10:32.785295\n",
      "Step  154 \t training loss: 1.4521484375 \t 13:10:43.567292\n",
      "Step  155 \t training loss: 1.359375 \t 13:10:54.355789\n",
      "Step  156 \t training loss: 1.3203125 \t 13:11:05.147132\n",
      "Step  157 \t training loss: 1.3115234375 \t 13:11:15.928731\n",
      "Step  158 \t training loss: 1.3505859375 \t 13:11:26.705480\n",
      "Step  159 \t training loss: 1.4482421875 \t 13:11:37.832906\n",
      "Step  160 \t training loss: 1.3125 \t 13:11:48.851735\n",
      "validation loss: 1.94140625\n",
      "Step  161 \t training loss: 1.3740234375 \t 13:12:00.225675\n",
      "Step  162 \t training loss: 1.328125 \t 13:12:11.002956\n",
      "Step  163 \t training loss: 1.291015625 \t 13:12:21.789276\n",
      "Step  164 \t training loss: 1.3671875 \t 13:12:32.573703\n",
      "Step  165 \t training loss: 1.2412109375 \t 13:12:43.357730\n",
      "Step  166 \t training loss: 1.3544921875 \t 13:12:54.138137\n",
      "Step  167 \t training loss: 1.3427734375 \t 13:13:05.262825\n",
      "Step  168 \t training loss: 1.2529296875 \t 13:13:16.279237\n",
      "Step  169 \t training loss: 1.2431640625 \t 13:13:27.069330\n",
      "Step  170 \t training loss: 1.28125 \t 13:13:37.853989\n",
      "Step  171 \t training loss: 1.2626953125 \t 13:13:48.634755\n",
      "Step  172 \t training loss: 1.3837890625 \t 13:13:59.421520\n",
      "Step  173 \t training loss: 1.3671875 \t 13:14:10.216356\n",
      "Step  174 \t training loss: 1.3525390625 \t 13:14:21.011110\n",
      "Step  175 \t training loss: 1.337890625 \t 13:14:32.185193\n",
      "Step  176 \t training loss: 1.365234375 \t 13:14:43.234221\n",
      "Step  177 \t training loss: 1.3505859375 \t 13:14:54.030160\n",
      "Step  178 \t training loss: 1.1015625 \t 13:15:04.819322\n",
      "Step  179 \t training loss: 1.263671875 \t 13:15:15.608171\n",
      "Step  180 \t training loss: 1.279296875 \t 13:15:26.393256\n",
      "validation loss: 1.994140625\n",
      "**************************************************************************************************** \n",
      "Question:  !What is 489.788 - 0.2?\"                                                                                                                                                                                                                                        \n",
      "Actual Answer:  !489.588\"                                                                                                                                                                                                                                                       \n",
      "Decoded Prediction:  \"g.8\"\"1\"\"\"1\"1\"@.0\"f.1\"\"1\".2\"\"\"\".\n",
      "Step  181 \t training loss: 1.3837890625 \t 13:15:39.923883\n",
      "Step  182 \t training loss: 1.3603515625 \t 13:15:50.705430\n",
      "Step  183 \t training loss: 1.322265625 \t 13:16:01.831026\n",
      "Step  184 \t training loss: 1.234375 \t 13:16:12.860551\n",
      "Step  185 \t training loss: 1.2451171875 \t 13:16:23.657801\n",
      "Step  186 \t training loss: 1.314453125 \t 13:16:34.440000\n",
      "Step  187 \t training loss: 1.2041015625 \t 13:16:45.223070\n",
      "Step  188 \t training loss: 1.189453125 \t 13:16:56.001883\n",
      "Step  189 \t training loss: 1.2412109375 \t 13:17:06.782342\n",
      "Step  190 \t training loss: 1.162109375 \t 13:17:17.569805\n",
      "Step  191 \t training loss: 1.283203125 \t 13:17:28.689880\n",
      "Step  192 \t training loss: 1.1611328125 \t 13:17:39.725977\n",
      "Step  193 \t training loss: 1.173828125 \t 13:17:50.509981\n",
      "Step  194 \t training loss: 1.154296875 \t 13:18:01.293822\n",
      "Step  195 \t training loss: 1.2109375 \t 13:18:12.076251\n",
      "Step  196 \t training loss: 1.2197265625 \t 13:18:22.863910\n",
      "Step  197 \t training loss: 1.1962890625 \t 13:18:33.652693\n",
      "Step  198 \t training loss: 1.1923828125 \t 13:18:44.445984\n",
      "Step  199 \t training loss: 1.2939453125 \t 13:18:55.569493\n",
      "Step  200 \t training loss: 1.1728515625 \t 13:19:06.599783\n",
      "validation loss: 1.9560546875\n",
      "Step  201 \t training loss: 1.119140625 \t 13:19:17.971893\n",
      "Step  202 \t training loss: 1.169921875 \t 13:19:28.759095\n",
      "Step  203 \t training loss: 1.2197265625 \t 13:19:39.545685\n",
      "Step  204 \t training loss: 1.17578125 \t 13:19:50.325279\n",
      "Step  205 \t training loss: 1.095703125 \t 13:20:01.104841\n",
      "Step  206 \t training loss: 1.1708984375 \t 13:20:11.886491\n",
      "Step  207 \t training loss: 1.1328125 \t 13:20:23.009792\n",
      "Step  208 \t training loss: 1.19921875 \t 13:20:34.054700\n",
      "Step  209 \t training loss: 1.16015625 \t 13:20:44.836676\n",
      "Step  210 \t training loss: 1.1064453125 \t 13:20:55.616130\n",
      "Step  211 \t training loss: 1.1611328125 \t 13:21:06.405428\n",
      "Step  212 \t training loss: 1.2373046875 \t 13:21:17.192588\n",
      "Step  213 \t training loss: 1.0869140625 \t 13:21:27.976909\n",
      "Step  214 \t training loss: 1.1005859375 \t 13:21:38.760153\n",
      "Step  215 \t training loss: 1.1298828125 \t 13:21:49.891969\n",
      "Step  216 \t training loss: 1.091796875 \t 13:22:00.945823\n",
      "Step  217 \t training loss: 1.1162109375 \t 13:22:11.855101\n",
      "Step  218 \t training loss: 0.98681640625 \t 13:22:22.736824\n",
      "Step  219 \t training loss: 1.0263671875 \t 13:22:33.554421\n",
      "Step  220 \t training loss: 1.2607421875 \t 13:22:44.452960\n",
      "validation loss: 2.04296875\n",
      "Step  221 \t training loss: 1.1171875 \t 13:22:56.387549\n",
      "Step  222 \t training loss: 1.025390625 \t 13:23:07.751745\n",
      "Step  223 \t training loss: 1.14453125 \t 13:23:19.559429\n",
      "Step  224 \t training loss: 0.92724609375 \t 13:23:31.209878\n",
      "Step  225 \t training loss: 0.998046875 \t 13:23:42.481379\n",
      "Step  226 \t training loss: 1.2451171875 \t 13:23:53.757480\n",
      "Step  227 \t training loss: 1.0361328125 \t 13:24:05.027114\n",
      "Step  228 \t training loss: 1.0986328125 \t 13:24:16.338009\n",
      "Step  229 \t training loss: 1.1396484375 \t 13:24:27.604688\n",
      "Step  230 \t training loss: 0.9677734375 \t 13:24:38.880419\n",
      "Step  231 \t training loss: 1.0380859375 \t 13:24:50.516474\n",
      "Step  232 \t training loss: 1.05078125 \t 13:25:02.058362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  233 \t training loss: 1.138671875 \t 13:25:13.349076\n",
      "Step  234 \t training loss: 0.9921875 \t 13:25:24.615732\n",
      "Step  235 \t training loss: 1.0693359375 \t 13:25:35.889804\n",
      "Step  236 \t training loss: 0.91259765625 \t 13:25:47.156579\n",
      "Step  237 \t training loss: 1.078125 \t 13:25:58.418637\n",
      "Step  238 \t training loss: 1.0400390625 \t 13:26:09.699172\n",
      "Step  239 \t training loss: 0.97509765625 \t 13:26:21.317379\n",
      "Step  240 \t training loss: 0.99072265625 \t 13:26:32.829511\n",
      "validation loss: 2.080078125\n",
      "**************************************************************************************************** \n",
      "Question:  !Evaluate (1 - 2) + -1 - (-2 - 1).\"                                                                                                                                                                                                                             \n",
      "Actual Answer:  !1\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  \"\"\"\"\"\"\"\"\"Q1\"\"\"\"\"\"\"\"\"\"\"\"1\"\"\"\"\"\"\"\"\n",
      "Step  241 \t training loss: 0.9892578125 \t 13:26:46.942033\n",
      "Step  242 \t training loss: 1.0166015625 \t 13:26:58.232606\n",
      "Step  243 \t training loss: 0.94140625 \t 13:27:09.555716\n",
      "Step  244 \t training loss: 1.0234375 \t 13:27:20.808461\n",
      "Step  245 \t training loss: 0.9736328125 \t 13:27:32.083670\n",
      "Step  246 \t training loss: 0.96484375 \t 13:27:43.345908\n",
      "Step  247 \t training loss: 0.96728515625 \t 13:27:54.958380\n",
      "Step  248 \t training loss: 0.82470703125 \t 13:28:06.500101\n",
      "Step  249 \t training loss: 0.92333984375 \t 13:28:17.785297\n",
      "Step  250 \t training loss: 0.94580078125 \t 13:28:29.056577\n",
      "Step  251 \t training loss: 0.9140625 \t 13:28:40.336050\n",
      "Step  252 \t training loss: 1.0546875 \t 13:28:51.613638\n",
      "Step  253 \t training loss: 1.044921875 \t 13:29:02.872411\n",
      "Step  254 \t training loss: 0.9150390625 \t 13:29:14.155684\n",
      "Step  255 \t training loss: 0.986328125 \t 13:29:25.835814\n",
      "Step  256 \t training loss: 0.89453125 \t 13:29:37.330035\n",
      "Step  257 \t training loss: 0.9775390625 \t 13:29:48.641429\n",
      "Step  258 \t training loss: 0.91943359375 \t 13:30:00.134226\n",
      "Step  259 \t training loss: 0.83837890625 \t 13:30:11.566626\n",
      "Step  260 \t training loss: 0.8251953125 \t 13:30:22.861759\n",
      "validation loss: 2.173828125\n",
      "Step  261 \t training loss: 0.880859375 \t 13:30:34.746764\n",
      "Step  262 \t training loss: 1.0498046875 \t 13:30:46.074149\n",
      "Step  263 \t training loss: 0.92919921875 \t 13:30:57.708859\n",
      "Step  264 \t training loss: 0.81689453125 \t 13:31:09.231803\n",
      "Step  265 \t training loss: 0.923828125 \t 13:31:20.502746\n",
      "Step  266 \t training loss: 0.7861328125 \t 13:31:31.784324\n",
      "Step  267 \t training loss: 0.7705078125 \t 13:31:43.037764\n",
      "Step  268 \t training loss: 0.8701171875 \t 13:31:54.315457\n",
      "Step  269 \t training loss: 0.94189453125 \t 13:32:05.581874\n",
      "Step  270 \t training loss: 0.966796875 \t 13:32:16.858016\n",
      "Step  271 \t training loss: 0.89794921875 \t 13:32:28.478934\n",
      "Step  272 \t training loss: 0.7998046875 \t 13:32:39.978043\n",
      "Step  273 \t training loss: 0.83154296875 \t 13:32:51.251352\n",
      "Step  274 \t training loss: 0.74267578125 \t 13:33:02.500989\n",
      "Step  275 \t training loss: 0.9072265625 \t 13:33:13.773454\n",
      "Step  276 \t training loss: 0.82666015625 \t 13:33:25.034399\n",
      "Step  277 \t training loss: 0.8681640625 \t 13:33:36.290594\n",
      "Step  278 \t training loss: 0.830078125 \t 13:33:47.571109\n",
      "Step  279 \t training loss: 0.81201171875 \t 13:33:59.213480\n",
      "Step  280 \t training loss: 0.7265625 \t 13:34:10.715317\n",
      "validation loss: 2.234375\n",
      "Step  281 \t training loss: 0.84423828125 \t 13:34:22.583409\n",
      "Step  282 \t training loss: 0.8408203125 \t 13:34:33.886005\n",
      "Step  283 \t training loss: 0.59912109375 \t 13:34:45.156358\n",
      "Step  284 \t training loss: 0.8505859375 \t 13:34:56.426627\n",
      "Step  285 \t training loss: 0.7958984375 \t 13:35:07.706371\n",
      "Step  286 \t training loss: 0.865234375 \t 13:35:18.959308\n",
      "Step  287 \t training loss: 0.75927734375 \t 13:35:30.569282\n",
      "Step  288 \t training loss: 0.81201171875 \t 13:35:42.099528\n",
      "Step  289 \t training loss: 0.716796875 \t 13:35:53.352353\n",
      "Step  290 \t training loss: 0.89697265625 \t 13:36:04.608549\n",
      "Step  291 \t training loss: 0.748046875 \t 13:36:15.881985\n",
      "Step  292 \t training loss: 0.69140625 \t 13:36:27.141315\n",
      "Step  293 \t training loss: 0.72509765625 \t 13:36:38.402274\n",
      "Step  294 \t training loss: 0.705078125 \t 13:36:49.679183\n",
      "Step  295 \t training loss: 0.83349609375 \t 13:37:01.300685\n",
      "Step  296 \t training loss: 0.7666015625 \t 13:37:12.815448\n",
      "Step  297 \t training loss: 0.6474609375 \t 13:37:24.089225\n",
      "Step  298 \t training loss: 0.8076171875 \t 13:37:35.349531\n",
      "Step  299 \t training loss: 0.69970703125 \t 13:37:46.590523\n",
      "Step  300 \t training loss: 0.73583984375 \t 13:37:57.860699\n",
      "validation loss: 2.470703125\n",
      "**************************************************************************************************** \n",
      "Question:  !1.93 + 2\"                                                                                                                                                                                                                                                      \n",
      "Actual Answer:  !3.93\"                                                                                                                                                                                                                                                          \n",
      "Decoded Prediction:  .\"\"\"\".8\"\"\"\"\"\"\"\"\"\"\"\"\"_\".<.1\"\"\"\"\"\"\n",
      "Step  301 \t training loss: 0.91015625 \t 13:38:11.905822\n",
      "Step  302 \t training loss: 0.75048828125 \t 13:38:23.166426\n",
      "Step  303 \t training loss: 0.67333984375 \t 13:38:34.809541\n",
      "Step  304 \t training loss: 0.6044921875 \t 13:38:46.333292\n",
      "Step  305 \t training loss: 0.72509765625 \t 13:38:57.610035\n",
      "Step  306 \t training loss: 0.6826171875 \t 13:39:08.878600\n",
      "Step  307 \t training loss: 0.78125 \t 13:39:20.155986\n",
      "Step  308 \t training loss: 0.75634765625 \t 13:39:31.405545\n",
      "Step  309 \t training loss: 0.6337890625 \t 13:39:42.657374\n",
      "Step  310 \t training loss: 0.84912109375 \t 13:39:53.924441\n",
      "Step  311 \t training loss: 0.7392578125 \t 13:40:05.545280\n",
      "Step  312 \t training loss: 0.5830078125 \t 13:40:17.079489\n",
      "Step  313 \t training loss: 0.6962890625 \t 13:40:28.363173\n",
      "Step  314 \t training loss: 0.63720703125 \t 13:40:39.630665\n",
      "Step  315 \t training loss: 0.67724609375 \t 13:40:50.886860\n",
      "Step  316 \t training loss: 0.73828125 \t 13:41:02.158955\n",
      "Step  317 \t training loss: 0.62841796875 \t 13:41:13.409983\n",
      "Step  318 \t training loss: 0.6748046875 \t 13:41:24.672270\n",
      "Step  319 \t training loss: 0.68505859375 \t 13:41:36.315189\n",
      "Step  320 \t training loss: 0.53369140625 \t 13:41:47.834979\n",
      "validation loss: 2.59375\n",
      "Step  321 \t training loss: 0.6767578125 \t 13:41:59.705489\n",
      "Step  322 \t training loss: 0.66357421875 \t 13:42:10.964791\n",
      "Step  323 \t training loss: 0.5751953125 \t 13:42:22.245002\n",
      "Step  324 \t training loss: 0.60693359375 \t 13:42:33.529268\n",
      "Step  325 \t training loss: 0.67919921875 \t 13:42:44.784007\n",
      "Step  326 \t training loss: 0.63818359375 \t 13:42:56.059724\n",
      "Step  327 \t training loss: 0.68017578125 \t 13:43:07.683172\n",
      "Step  328 \t training loss: 0.6259765625 \t 13:43:19.198566\n",
      "Step  329 \t training loss: 0.62255859375 \t 13:43:30.457623\n",
      "Step  330 \t training loss: 0.5185546875 \t 13:43:41.726569\n",
      "Step  331 \t training loss: 0.59716796875 \t 13:43:52.990832\n",
      "Step  332 \t training loss: 0.5166015625 \t 13:44:04.265585\n",
      "Step  333 \t training loss: 0.6767578125 \t 13:44:15.528153\n",
      "Step  334 \t training loss: 0.63330078125 \t 13:44:26.797133\n",
      "Step  335 \t training loss: 0.5888671875 \t 13:44:38.425908\n",
      "Step  336 \t training loss: 0.6005859375 \t 13:44:49.951485\n",
      "Step  337 \t training loss: 0.51220703125 \t 13:45:01.214530\n",
      "Step  338 \t training loss: 0.515625 \t 13:45:12.475702\n",
      "Step  339 \t training loss: 0.52294921875 \t 13:45:23.745202\n",
      "Step  340 \t training loss: 0.5546875 \t 13:45:34.998165\n",
      "validation loss: 2.67578125\n",
      "Step  341 \t training loss: 0.548828125 \t 13:45:46.859465\n",
      "Step  342 \t training loss: 0.59130859375 \t 13:45:58.131945\n",
      "Step  343 \t training loss: 0.53173828125 \t 13:46:09.769727\n",
      "Step  344 \t training loss: 0.442626953125 \t 13:46:21.294895\n",
      "Step  345 \t training loss: 0.430908203125 \t 13:46:32.547660\n",
      "Step  346 \t training loss: 0.517578125 \t 13:46:43.820131\n",
      "Step  347 \t training loss: 0.50634765625 \t 13:46:55.086202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  348 \t training loss: 0.442138671875 \t 13:47:06.357415\n",
      "Step  349 \t training loss: 0.48291015625 \t 13:47:17.623646\n",
      "Step  350 \t training loss: 0.4736328125 \t 13:47:28.890769\n",
      "Step  351 \t training loss: 0.517578125 \t 13:47:40.509959\n",
      "Step  352 \t training loss: 0.37451171875 \t 13:47:51.998846\n",
      "Step  353 \t training loss: 0.474609375 \t 13:48:03.277224\n",
      "Step  354 \t training loss: 0.374267578125 \t 13:48:14.530751\n",
      "Step  355 \t training loss: 0.423828125 \t 13:48:25.815973\n",
      "Step  356 \t training loss: 0.40478515625 \t 13:48:37.101472\n",
      "Step  357 \t training loss: 0.49609375 \t 13:48:48.595111\n",
      "Step  358 \t training loss: 0.4482421875 \t 13:49:00.025945\n",
      "Step  359 \t training loss: 0.494140625 \t 13:49:11.772317\n",
      "Step  360 \t training loss: 0.39892578125 \t 13:49:23.289533\n",
      "validation loss: 2.884765625\n",
      "**************************************************************************************************** \n",
      "Question:  !Evaluate 0 - 9 - (-4 - -1).\"                                                                                                                                                                                                                                   \n",
      "Actual Answer:  !-6\"                                                                                                                                                                                                                                                            \n",
      "Decoded Prediction:  \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "Step  361 \t training loss: 0.401611328125 \t 13:49:37.344468\n",
      "Step  362 \t training loss: 0.427001953125 \t 13:49:48.592514\n",
      "Step  363 \t training loss: 0.40234375 \t 13:49:59.857432\n",
      "Step  364 \t training loss: 0.430419921875 \t 13:50:11.106858\n",
      "Step  365 \t training loss: 0.418701171875 \t 13:50:22.383411\n",
      "Step  366 \t training loss: 0.370361328125 \t 13:50:33.659751\n",
      "Step  367 \t training loss: 0.4931640625 \t 13:50:45.284433\n",
      "Step  368 \t training loss: 0.349609375 \t 13:50:56.796836\n",
      "Step  369 \t training loss: 0.44140625 \t 13:51:08.069171\n",
      "Step  370 \t training loss: 0.3955078125 \t 13:51:19.325577\n",
      "Step  371 \t training loss: 0.338134765625 \t 13:51:30.575353\n",
      "Step  372 \t training loss: 0.383056640625 \t 13:51:41.847843\n",
      "Step  373 \t training loss: 0.408447265625 \t 13:51:53.095962\n",
      "Step  374 \t training loss: 0.458251953125 \t 13:52:04.365120\n",
      "Step  375 \t training loss: 0.365966796875 \t 13:52:16.033161\n",
      "Step  376 \t training loss: 0.335205078125 \t 13:52:27.519188\n",
      "Step  377 \t training loss: 0.34912109375 \t 13:52:38.800653\n",
      "Step  378 \t training loss: 0.4931640625 \t 13:52:50.079781\n",
      "Step  379 \t training loss: 0.37744140625 \t 13:53:01.341082\n",
      "Step  380 \t training loss: 0.302490234375 \t 13:53:12.610513\n",
      "validation loss: 2.9375\n",
      "Step  381 \t training loss: 0.433349609375 \t 13:53:24.486838\n",
      "Step  382 \t training loss: 0.394287109375 \t 13:53:35.750986\n",
      "Step  383 \t training loss: 0.323486328125 \t 13:53:47.377086\n",
      "Step  384 \t training loss: 0.375732421875 \t 13:53:58.890321\n",
      "Step  385 \t training loss: 0.301513671875 \t 13:54:10.218389\n",
      "Step  386 \t training loss: 0.27294921875 \t 13:54:21.627518\n",
      "Step  387 \t training loss: 0.24560546875 \t 13:54:32.923943\n",
      "Step  388 \t training loss: 0.348876953125 \t 13:54:44.383186\n",
      "Step  389 \t training loss: 0.287841796875 \t 13:54:55.678299\n",
      "Step  390 \t training loss: 0.34228515625 \t 13:55:06.938840\n",
      "Step  391 \t training loss: 0.3525390625 \t 13:55:18.557223\n",
      "Step  392 \t training loss: 0.30029296875 \t 13:55:30.087689\n",
      "Step  393 \t training loss: 0.262939453125 \t 13:55:41.351554\n",
      "Step  394 \t training loss: 0.232421875 \t 13:55:52.609957\n",
      "Step  395 \t training loss: 0.238037109375 \t 13:56:03.884338\n",
      "Step  396 \t training loss: 0.329833984375 \t 13:56:15.153948\n",
      "Step  397 \t training loss: 0.25390625 \t 13:56:26.419304\n",
      "Step  398 \t training loss: 0.281005859375 \t 13:56:37.725336\n",
      "Step  399 \t training loss: 0.26123046875 \t 13:56:49.345874\n",
      "Step  400 \t training loss: 0.2431640625 \t 13:57:00.862266\n",
      "validation loss: 3.1171875\n",
      "Step  401 \t training loss: 0.227294921875 \t 13:57:12.740349\n",
      "Step  402 \t training loss: 0.211181640625 \t 13:57:24.021248\n",
      "Step  403 \t training loss: 0.266357421875 \t 13:57:35.282698\n",
      "Step  404 \t training loss: 0.282470703125 \t 13:57:46.540974\n",
      "Step  405 \t training loss: 0.256591796875 \t 13:57:57.820138\n",
      "Step  406 \t training loss: 0.28515625 \t 13:58:09.080523\n",
      "Step  407 \t training loss: 0.2332763671875 \t 13:58:20.702134\n",
      "Step  408 \t training loss: 0.1817626953125 \t 13:58:32.214168\n",
      "Step  409 \t training loss: 0.235595703125 \t 13:58:43.486483\n",
      "Step  410 \t training loss: 0.202880859375 \t 13:58:54.742367\n",
      "Step  411 \t training loss: 0.21875 \t 13:59:06.013025\n",
      "Step  412 \t training loss: 0.2232666015625 \t 13:59:17.280005\n",
      "Step  413 \t training loss: 0.2122802734375 \t 13:59:28.538614\n",
      "Step  414 \t training loss: 0.23681640625 \t 13:59:39.817630\n",
      "Step  415 \t training loss: 0.2265625 \t 13:59:51.450161\n",
      "Step  416 \t training loss: 0.2100830078125 \t 14:00:02.945845\n",
      "Step  417 \t training loss: 0.175537109375 \t 14:00:14.224511\n",
      "Step  418 \t training loss: 0.2149658203125 \t 14:00:25.501559\n",
      "Step  419 \t training loss: 0.2000732421875 \t 14:00:36.763942\n",
      "Step  420 \t training loss: 0.197021484375 \t 14:00:48.036173\n",
      "validation loss: 3.330078125\n",
      "**************************************************************************************************** \n",
      "Question:  !What is -2.8 - 27.8?\"                                                                                                                                                                                                                                          \n",
      "Actual Answer:  !-30.6\"                                                                                                                                                                                                                                                         \n",
      "Decoded Prediction:  \"\"\"\"\"\".8.\"8\"8\".\"\"%\"\"1\"\"\"\"\"\".\"\"\"\"\n",
      "Step  421 \t training loss: 0.276611328125 \t 14:01:02.203724\n",
      "Step  422 \t training loss: 0.1871337890625 \t 14:01:13.468557\n",
      "Step  423 \t training loss: 0.176025390625 \t 14:01:25.106946\n",
      "Step  424 \t training loss: 0.193603515625 \t 14:01:36.626635\n",
      "Step  425 \t training loss: 0.185302734375 \t 14:01:47.898659\n",
      "Step  426 \t training loss: 0.2032470703125 \t 14:01:59.154784\n",
      "Step  427 \t training loss: 0.190673828125 \t 14:02:10.424854\n",
      "Step  428 \t training loss: 0.195068359375 \t 14:02:21.703716\n",
      "Step  429 \t training loss: 0.1640625 \t 14:02:32.969135\n",
      "Step  430 \t training loss: 0.21435546875 \t 14:02:44.255735\n",
      "Step  431 \t training loss: 0.1705322265625 \t 14:02:55.884314\n",
      "Step  432 \t training loss: 0.1507568359375 \t 14:03:07.399636\n",
      "Step  433 \t training loss: 0.183837890625 \t 14:03:18.654738\n",
      "Step  434 \t training loss: 0.2158203125 \t 14:03:29.915933\n",
      "Step  435 \t training loss: 0.20556640625 \t 14:03:41.177293\n",
      "Step  436 \t training loss: 0.130615234375 \t 14:03:52.449004\n",
      "Step  437 \t training loss: 0.149169921875 \t 14:04:03.724968\n",
      "Step  438 \t training loss: 0.1474609375 \t 14:04:14.979191\n",
      "Step  439 \t training loss: 0.1439208984375 \t 14:04:26.624990\n",
      "Step  440 \t training loss: 0.154052734375 \t 14:04:38.157100\n",
      "validation loss: 3.51953125\n",
      "Step  441 \t training loss: 0.1484375 \t 14:04:50.024890\n",
      "Step  442 \t training loss: 0.165283203125 \t 14:05:01.298999\n",
      "Step  443 \t training loss: 0.1485595703125 \t 14:05:12.562654\n",
      "Step  444 \t training loss: 0.1553955078125 \t 14:05:23.847508\n",
      "Step  445 \t training loss: 0.1387939453125 \t 14:05:35.115394\n",
      "Step  446 \t training loss: 0.1435546875 \t 14:05:46.385651\n",
      "Step  447 \t training loss: 0.1505126953125 \t 14:05:58.029443\n",
      "Step  448 \t training loss: 0.10443115234375 \t 14:06:09.545091\n",
      "Step  449 \t training loss: 0.1513671875 \t 14:06:20.823096\n",
      "Step  450 \t training loss: 0.10833740234375 \t 14:06:32.119742\n",
      "Step  451 \t training loss: 0.13671875 \t 14:06:43.561587\n",
      "Step  452 \t training loss: 0.10089111328125 \t 14:06:54.831574\n",
      "Step  453 \t training loss: 0.1636962890625 \t 14:07:06.181881\n",
      "Step  454 \t training loss: 0.1318359375 \t 14:07:17.464379\n",
      "Step  455 \t training loss: 0.1488037109375 \t 14:07:29.159052\n",
      "Step  456 \t training loss: 0.10345458984375 \t 14:07:40.766472\n",
      "Step  457 \t training loss: 0.1046142578125 \t 14:07:52.149633\n",
      "Step  458 \t training loss: 0.10186767578125 \t 14:08:02.865655\n",
      "Step  459 \t training loss: 0.10894775390625 \t 14:08:13.584882\n",
      "Step  460 \t training loss: 0.09930419921875 \t 14:08:24.306684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 3.7421875\n",
      "Step  461 \t training loss: 0.11285400390625 \t 14:08:35.597973\n",
      "Step  462 \t training loss: 0.1070556640625 \t 14:08:46.317569\n",
      "Step  463 \t training loss: 0.09954833984375 \t 14:08:57.373612\n",
      "Step  464 \t training loss: 0.09674072265625 \t 14:09:08.319906\n",
      "Step  465 \t training loss: 0.0933837890625 \t 14:09:19.042694\n",
      "Step  466 \t training loss: 0.09600830078125 \t 14:09:29.755829\n",
      "Step  467 \t training loss: 0.10028076171875 \t 14:09:40.471524\n",
      "Step  468 \t training loss: 0.0972900390625 \t 14:09:51.188799\n",
      "Step  469 \t training loss: 0.0972900390625 \t 14:10:01.903920\n",
      "Step  470 \t training loss: 0.11077880859375 \t 14:10:12.623850\n",
      "Step  471 \t training loss: 0.08154296875 \t 14:10:23.673003\n",
      "Step  472 \t training loss: 0.0887451171875 \t 14:10:34.624689\n",
      "Step  473 \t training loss: 0.06329345703125 \t 14:10:45.344661\n",
      "Step  474 \t training loss: 0.0938720703125 \t 14:10:56.069262\n",
      "Step  475 \t training loss: 0.09051513671875 \t 14:11:06.795319\n",
      "Step  476 \t training loss: 0.077880859375 \t 14:11:17.511496\n",
      "Step  477 \t training loss: 0.1025390625 \t 14:11:28.224840\n",
      "Step  478 \t training loss: 0.089111328125 \t 14:11:38.938443\n",
      "Step  479 \t training loss: 0.1009521484375 \t 14:11:49.983818\n",
      "Step  480 \t training loss: 0.0635986328125 \t 14:12:00.959968\n",
      "validation loss: 3.787109375\n",
      "**************************************************************************************************** \n",
      "Question:  !Work out -0.3 - -433.\"                                                                                                                                                                                                                                         \n",
      "Actual Answer:  !432.7\"                                                                                                                                                                                                                                                         \n",
      "Decoded Prediction:  .1\"\"\"\"\"\"R.\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" \"\"\"\"\"\n",
      "Step  481 \t training loss: 0.06787109375 \t 14:12:14.408696\n",
      "Step  482 \t training loss: 0.0775146484375 \t 14:12:25.126681\n",
      "Step  483 \t training loss: 0.05975341796875 \t 14:12:35.839490\n",
      "Step  484 \t training loss: 0.06475830078125 \t 14:12:46.551940\n",
      "Step  485 \t training loss: 0.081787109375 \t 14:12:57.266370\n",
      "Step  486 \t training loss: 0.0667724609375 \t 14:13:07.975963\n",
      "Step  487 \t training loss: 0.06121826171875 \t 14:13:19.021668\n",
      "Step  488 \t training loss: 0.0567626953125 \t 14:13:29.975038\n",
      "Step  489 \t training loss: 0.059326171875 \t 14:13:40.702952\n",
      "Step  490 \t training loss: 0.05633544921875 \t 14:13:51.428060\n",
      "Step  491 \t training loss: 0.054107666015625 \t 14:14:02.152603\n",
      "Step  492 \t training loss: 0.06097412109375 \t 14:14:12.866208\n",
      "Step  493 \t training loss: 0.05438232421875 \t 14:14:23.584476\n",
      "Step  494 \t training loss: 0.0672607421875 \t 14:14:34.301985\n",
      "Step  495 \t training loss: 0.061767578125 \t 14:14:45.350380\n",
      "Step  496 \t training loss: 0.048858642578125 \t 14:14:56.325956\n",
      "Step  497 \t training loss: 0.054107666015625 \t 14:15:07.043146\n",
      "Step  498 \t training loss: 0.05035400390625 \t 14:15:17.774199\n",
      "Step  499 \t training loss: 0.054901123046875 \t 14:15:28.497781\n",
      "Step  500 \t training loss: 0.05035400390625 \t 14:15:39.559259\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "validation loss: 4.00390625\n",
      "Step  501 \t training loss: 0.04791259765625 \t 14:15:51.452333\n",
      "Step  502 \t training loss: 0.057220458984375 \t 14:16:02.983304\n",
      "Step  503 \t training loss: 0.067138671875 \t 14:16:14.647601\n",
      "Step  504 \t training loss: 0.048065185546875 \t 14:16:26.185728\n",
      "Step  505 \t training loss: 0.0450439453125 \t 14:16:37.449922\n",
      "Step  506 \t training loss: 0.058380126953125 \t 14:16:48.707670\n",
      "Step  507 \t training loss: 0.06103515625 \t 14:16:59.979499\n",
      "Step  508 \t training loss: 0.0447998046875 \t 14:17:11.251244\n",
      "Step  509 \t training loss: 0.0538330078125 \t 14:17:22.514488\n",
      "Step  510 \t training loss: 0.053863525390625 \t 14:17:33.781905\n",
      "Step  511 \t training loss: 0.05548095703125 \t 14:17:45.400651\n",
      "Step  512 \t training loss: 0.034515380859375 \t 14:17:56.908390\n",
      "Step  513 \t training loss: 0.044281005859375 \t 14:18:08.194460\n",
      "Step  514 \t training loss: 0.05035400390625 \t 14:18:19.485014\n",
      "Step  515 \t training loss: 0.05633544921875 \t 14:18:30.741594\n",
      "Step  516 \t training loss: 0.042388916015625 \t 14:18:42.018810\n",
      "Step  517 \t training loss: 0.05352783203125 \t 14:18:53.280390\n",
      "Step  518 \t training loss: 0.04461669921875 \t 14:19:04.540525\n",
      "Step  519 \t training loss: 0.056060791015625 \t 14:19:16.181463\n",
      "Step  520 \t training loss: 0.042236328125 \t 14:19:27.728221\n",
      "validation loss: 4.16015625\n",
      "Step  521 \t training loss: 0.044403076171875 \t 14:19:39.584889\n",
      "Step  522 \t training loss: 0.04046630859375 \t 14:19:50.854431\n",
      "Step  523 \t training loss: 0.045989990234375 \t 14:20:02.130876\n",
      "Step  524 \t training loss: 0.042144775390625 \t 14:20:13.375215\n",
      "Step  525 \t training loss: 0.04461669921875 \t 14:20:24.632765\n",
      "Step  526 \t training loss: 0.04296875 \t 14:20:35.909446\n",
      "Step  527 \t training loss: 0.037261962890625 \t 14:20:47.533001\n",
      "Step  528 \t training loss: 0.033477783203125 \t 14:20:59.057188\n",
      "Step  529 \t training loss: 0.038848876953125 \t 14:21:10.328774\n",
      "Step  530 \t training loss: 0.042633056640625 \t 14:21:21.592302\n",
      "Step  531 \t training loss: 0.03887939453125 \t 14:21:32.849662\n",
      "Step  532 \t training loss: 0.0270233154296875 \t 14:21:44.131516\n",
      "Step  533 \t training loss: 0.04461669921875 \t 14:21:55.390334\n",
      "Step  534 \t training loss: 0.044769287109375 \t 14:22:06.634375\n",
      "Step  535 \t training loss: 0.032440185546875 \t 14:22:18.282478\n",
      "Step  536 \t training loss: 0.0340576171875 \t 14:22:29.798633\n",
      "Step  537 \t training loss: 0.0265960693359375 \t 14:22:41.064666\n",
      "Step  538 \t training loss: 0.0330810546875 \t 14:22:52.340253\n",
      "Step  539 \t training loss: 0.02886962890625 \t 14:23:03.607393\n",
      "Step  540 \t training loss: 0.033538818359375 \t 14:23:14.872372\n",
      "validation loss: 4.25390625\n",
      "**************************************************************************************************** \n",
      "Question:  !Work out 15 + 908.2.\"                                                                                                                                                                                                                                          \n",
      "Actual Answer:  !923.2\"                                                                                                                                                                                                                                                         \n",
      "Decoded Prediction:  .\".\"\"\"\"1\"\"\"\"\"\"8.\".\"\".8.\"\"\"\"\"@.\"\"\n",
      "Step  541 \t training loss: 0.05706787109375 \t 14:23:29.025086\n",
      "Step  542 \t training loss: 0.026947021484375 \t 14:23:40.294989\n",
      "Step  543 \t training loss: 0.02911376953125 \t 14:23:51.928430\n",
      "Step  544 \t training loss: 0.032135009765625 \t 14:24:03.461093\n",
      "Step  545 \t training loss: 0.034271240234375 \t 14:24:14.723419\n",
      "Step  546 \t training loss: 0.0292816162109375 \t 14:24:26.012395\n",
      "Step  547 \t training loss: 0.03485107421875 \t 14:24:37.276961\n",
      "Step  548 \t training loss: 0.0298614501953125 \t 14:24:48.539611\n",
      "Step  549 \t training loss: 0.037933349609375 \t 14:24:59.813018\n",
      "Step  550 \t training loss: 0.03076171875 \t 14:25:11.070937\n",
      "Step  551 \t training loss: 0.0220947265625 \t 14:25:22.688332\n",
      "Step  552 \t training loss: 0.0338134765625 \t 14:25:34.217458\n",
      "Step  553 \t training loss: 0.0292510986328125 \t 14:25:45.491751\n",
      "Step  554 \t training loss: 0.03265380859375 \t 14:25:56.747870\n",
      "Step  555 \t training loss: 0.0214385986328125 \t 14:26:08.014798\n",
      "Step  556 \t training loss: 0.026702880859375 \t 14:26:19.293913\n",
      "Step  557 \t training loss: 0.022491455078125 \t 14:26:30.550561\n",
      "Step  558 \t training loss: 0.0279541015625 \t 14:26:41.829815\n",
      "Step  559 \t training loss: 0.029632568359375 \t 14:26:53.463640\n",
      "Step  560 \t training loss: 0.0219573974609375 \t 14:27:04.960485\n",
      "validation loss: 4.37109375\n",
      "Step  561 \t training loss: 0.0234832763671875 \t 14:27:16.803617\n",
      "Step  562 \t training loss: 0.0249176025390625 \t 14:27:28.082970\n",
      "Step  563 \t training loss: 0.0191802978515625 \t 14:27:39.353012\n",
      "Step  564 \t training loss: 0.0208892822265625 \t 14:27:50.596128\n",
      "Step  565 \t training loss: 0.0265960693359375 \t 14:28:01.875477\n",
      "Step  566 \t training loss: 0.0244598388671875 \t 14:28:13.134589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  567 \t training loss: 0.0236968994140625 \t 14:28:24.769987\n",
      "Step  568 \t training loss: 0.01898193359375 \t 14:28:36.286266\n",
      "Step  569 \t training loss: 0.0260772705078125 \t 14:28:47.558884\n",
      "Step  570 \t training loss: 0.0188751220703125 \t 14:28:58.815536\n",
      "Step  571 \t training loss: 0.0176544189453125 \t 14:29:10.104330\n",
      "Step  572 \t training loss: 0.0218658447265625 \t 14:29:21.356177\n",
      "Step  573 \t training loss: 0.0207672119140625 \t 14:29:32.606718\n",
      "Step  574 \t training loss: 0.022705078125 \t 14:29:43.908837\n",
      "Step  575 \t training loss: 0.019287109375 \t 14:29:55.532253\n",
      "Step  576 \t training loss: 0.0186614990234375 \t 14:30:07.032802\n",
      "Step  577 \t training loss: 0.0145416259765625 \t 14:30:18.306217\n",
      "Step  578 \t training loss: 0.0174102783203125 \t 14:30:29.581847\n",
      "Step  579 \t training loss: 0.02154541015625 \t 14:30:40.839236\n",
      "Step  580 \t training loss: 0.0182647705078125 \t 14:30:52.114501\n",
      "validation loss: 4.4140625\n",
      "Step  581 \t training loss: 0.020477294921875 \t 14:31:03.976047\n",
      "Step  582 \t training loss: 0.02398681640625 \t 14:31:15.230806\n",
      "Step  583 \t training loss: 0.016571044921875 \t 14:31:26.858045\n",
      "Step  584 \t training loss: 0.022125244140625 \t 14:31:38.452091\n",
      "Step  585 \t training loss: 0.0162506103515625 \t 14:31:49.834190\n",
      "Step  586 \t training loss: 0.02020263671875 \t 14:32:01.189266\n",
      "Step  587 \t training loss: 0.0157928466796875 \t 14:32:12.464778\n",
      "Step  588 \t training loss: 0.0156707763671875 \t 14:32:23.761226\n",
      "Step  589 \t training loss: 0.014556884765625 \t 14:32:35.009406\n",
      "Step  590 \t training loss: 0.0235443115234375 \t 14:32:46.284703\n",
      "Step  591 \t training loss: 0.0238800048828125 \t 14:32:57.922696\n",
      "Step  592 \t training loss: 0.014556884765625 \t 14:33:09.470703\n",
      "Step  593 \t training loss: 0.0153961181640625 \t 14:33:20.736109\n",
      "Step  594 \t training loss: 0.016571044921875 \t 14:33:32.009968\n",
      "Step  595 \t training loss: 0.0142059326171875 \t 14:33:43.267103\n",
      "Step  596 \t training loss: 0.0201568603515625 \t 14:33:54.526000\n",
      "Step  597 \t training loss: 0.01959228515625 \t 14:34:05.787418\n",
      "Step  598 \t training loss: 0.01404571533203125 \t 14:34:17.040555\n",
      "Step  599 \t training loss: 0.0203399658203125 \t 14:34:28.691024\n",
      "Step  600 \t training loss: 0.01543426513671875 \t 14:34:40.184433\n",
      "validation loss: 4.55859375\n",
      "**************************************************************************************************** \n",
      "Question:  !What is 0.4 plus 0.455?\"                                                                                                                                                                                                                                       \n",
      "Actual Answer:  !0.855\"                                                                                                                                                                                                                                                         \n",
      "Decoded Prediction:  .\".\".\"\"\"\"\"\"\".\".\"\".\"\"\"\".\"\"\"\"R.\"\".\n",
      "Step  601 \t training loss: 0.0145111083984375 \t 14:34:54.234893\n",
      "Step  602 \t training loss: 0.0162506103515625 \t 14:35:05.496186\n",
      "Step  603 \t training loss: 0.017730712890625 \t 14:35:16.755837\n",
      "Step  604 \t training loss: 0.0144805908203125 \t 14:35:28.043235\n",
      "Step  605 \t training loss: 0.0157928466796875 \t 14:35:39.309905\n",
      "Step  606 \t training loss: 0.01548004150390625 \t 14:35:50.564058\n",
      "Step  607 \t training loss: 0.0213165283203125 \t 14:36:02.215612\n",
      "Step  608 \t training loss: 0.01617431640625 \t 14:36:13.733334\n",
      "Step  609 \t training loss: 0.014190673828125 \t 14:36:24.993516\n",
      "Step  610 \t training loss: 0.0160675048828125 \t 14:36:36.263931\n",
      "Step  611 \t training loss: 0.0140838623046875 \t 14:36:47.524884\n",
      "Step  612 \t training loss: 0.0184173583984375 \t 14:36:58.776620\n",
      "Step  613 \t training loss: 0.0176849365234375 \t 14:37:10.046546\n",
      "Step  614 \t training loss: 0.017791748046875 \t 14:37:21.312026\n",
      "Step  615 \t training loss: 0.0142669677734375 \t 14:37:32.939161\n",
      "Step  616 \t training loss: 0.0152587890625 \t 14:37:44.460843\n",
      "Step  617 \t training loss: 0.017181396484375 \t 14:37:55.747123\n",
      "Step  618 \t training loss: 0.0126495361328125 \t 14:38:06.986033\n",
      "Step  619 \t training loss: 0.015380859375 \t 14:38:18.255021\n",
      "Step  620 \t training loss: 0.01324462890625 \t 14:38:29.535147\n",
      "validation loss: 4.63671875\n",
      "Step  621 \t training loss: 0.017852783203125 \t 14:38:41.394715\n",
      "Step  622 \t training loss: 0.011993408203125 \t 14:38:52.653619\n",
      "Step  623 \t training loss: 0.0108489990234375 \t 14:39:04.299753\n",
      "Step  624 \t training loss: 0.01096343994140625 \t 14:39:15.811273\n",
      "Step  625 \t training loss: 0.01378631591796875 \t 14:39:27.070478\n",
      "Step  626 \t training loss: 0.01080322265625 \t 14:39:38.351871\n",
      "Step  627 \t training loss: 0.013458251953125 \t 14:39:49.647964\n",
      "Step  628 \t training loss: 0.00994110107421875 \t 14:40:01.034292\n",
      "Step  629 \t training loss: 0.01346588134765625 \t 14:40:12.467468\n",
      "Step  630 \t training loss: 0.01262664794921875 \t 14:40:23.825491\n",
      "Step  631 \t training loss: 0.0128326416015625 \t 14:40:35.449995\n",
      "Step  632 \t training loss: 0.0120391845703125 \t 14:40:46.949847\n",
      "Step  633 \t training loss: 0.01296234130859375 \t 14:40:58.225712\n",
      "Step  634 \t training loss: 0.01250457763671875 \t 14:41:09.502528\n",
      "Step  635 \t training loss: 0.0151214599609375 \t 14:41:20.814233\n",
      "Step  636 \t training loss: 0.0130462646484375 \t 14:41:32.129709\n",
      "Step  637 \t training loss: 0.01457977294921875 \t 14:41:43.384789\n",
      "Step  638 \t training loss: 0.0125732421875 \t 14:41:54.658017\n",
      "Step  639 \t training loss: 0.01092529296875 \t 14:42:06.298329\n",
      "Step  640 \t training loss: 0.01025390625 \t 14:42:17.816250\n",
      "validation loss: 4.8125\n",
      "Step  641 \t training loss: 0.0092315673828125 \t 14:42:29.682045\n",
      "Step  642 \t training loss: 0.01215362548828125 \t 14:42:40.935974\n",
      "Step  643 \t training loss: 0.0137786865234375 \t 14:42:52.206455\n",
      "Step  644 \t training loss: 0.01068878173828125 \t 14:43:03.471093\n",
      "Step  645 \t training loss: 0.0126495361328125 \t 14:43:14.728492\n",
      "Step  646 \t training loss: 0.01141357421875 \t 14:43:26.000360\n",
      "Step  647 \t training loss: 0.013275146484375 \t 14:43:37.622026\n",
      "Step  648 \t training loss: 0.01043701171875 \t 14:43:49.136741\n",
      "Step  649 \t training loss: 0.01103973388671875 \t 14:44:00.399405\n",
      "Step  650 \t training loss: 0.0116424560546875 \t 14:44:11.664782\n",
      "Step  651 \t training loss: 0.01288604736328125 \t 14:44:22.944204\n",
      "Step  652 \t training loss: 0.00905609130859375 \t 14:44:34.209824\n",
      "Step  653 \t training loss: 0.00933837890625 \t 14:44:45.461679\n",
      "Step  654 \t training loss: 0.01180267333984375 \t 14:44:56.733408\n",
      "Step  655 \t training loss: 0.00962066650390625 \t 14:45:08.362615\n",
      "Step  656 \t training loss: 0.00948333740234375 \t 14:45:19.870919\n",
      "Step  657 \t training loss: 0.010040283203125 \t 14:45:31.129838\n",
      "Step  658 \t training loss: 0.00768280029296875 \t 14:45:42.392782\n",
      "Step  659 \t training loss: 0.01090240478515625 \t 14:45:53.673443\n",
      "Step  660 \t training loss: 0.0087738037109375 \t 14:46:04.933128\n",
      "validation loss: 4.9375\n",
      "**************************************************************************************************** \n",
      "Question:  !What is 0.0151 less than 0.6?\"                                                                                                                                                                                                                                 \n",
      "Actual Answer:  !0.5849\"                                                                                                                                                                                                                                                        \n",
      "Decoded Prediction:  .8\".\".\"\"\".\"\".\".\".\".8.\"\"\"\".\".\"\".\"\n",
      "Step  661 \t training loss: 0.01171112060546875 \t 14:46:18.966411\n",
      "Step  662 \t training loss: 0.0116424560546875 \t 14:46:30.249560\n",
      "Step  663 \t training loss: 0.0116729736328125 \t 14:46:41.879231\n",
      "Step  664 \t training loss: 0.01480865478515625 \t 14:46:53.384281\n",
      "Step  665 \t training loss: 0.0110015869140625 \t 14:47:04.661581\n",
      "Step  666 \t training loss: 0.00942230224609375 \t 14:47:15.938502\n",
      "Step  667 \t training loss: 0.0114288330078125 \t 14:47:27.190747\n",
      "Step  668 \t training loss: 0.0082855224609375 \t 14:47:38.456636\n",
      "Step  669 \t training loss: 0.0079803466796875 \t 14:47:49.741698\n",
      "Step  670 \t training loss: 0.0104217529296875 \t 14:48:00.996520\n",
      "Step  671 \t training loss: 0.0089874267578125 \t 14:48:12.619411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  672 \t training loss: 0.00855255126953125 \t 14:48:24.186055\n",
      "Step  673 \t training loss: 0.01183319091796875 \t 14:48:35.437621\n",
      "Step  674 \t training loss: 0.0077972412109375 \t 14:48:46.685115\n",
      "Step  675 \t training loss: 0.00945281982421875 \t 14:48:57.966348\n",
      "Step  676 \t training loss: 0.00927734375 \t 14:49:09.225238\n",
      "Step  677 \t training loss: 0.00905609130859375 \t 14:49:20.483629\n",
      "Step  678 \t training loss: 0.01006317138671875 \t 14:49:31.753107\n",
      "Step  679 \t training loss: 0.01012420654296875 \t 14:49:43.372958\n",
      "Step  680 \t training loss: 0.01044464111328125 \t 14:49:54.857704\n",
      "validation loss: 4.92578125\n",
      "Step  681 \t training loss: 0.01110076904296875 \t 14:50:06.733393\n",
      "Step  682 \t training loss: 0.00936126708984375 \t 14:50:17.999037\n",
      "Step  683 \t training loss: 0.00799560546875 \t 14:50:29.264489\n",
      "Step  684 \t training loss: 0.008880615234375 \t 14:50:40.583978\n",
      "Step  685 \t training loss: 0.00878143310546875 \t 14:50:51.916399\n",
      "Step  686 \t training loss: 0.0090789794921875 \t 14:51:03.171873\n",
      "Step  687 \t training loss: 0.00838470458984375 \t 14:51:14.806070\n",
      "Step  688 \t training loss: 0.007541656494140625 \t 14:51:26.331067\n",
      "Step  689 \t training loss: 0.0084381103515625 \t 14:51:37.594214\n",
      "Step  690 \t training loss: 0.006885528564453125 \t 14:51:48.926164\n",
      "Step  691 \t training loss: 0.00688934326171875 \t 14:52:00.380136\n",
      "Step  692 \t training loss: 0.0078582763671875 \t 14:52:11.726141\n",
      "Step  693 \t training loss: 0.01346588134765625 \t 14:52:22.994191\n",
      "Step  694 \t training loss: 0.005115509033203125 \t 14:52:34.273066\n",
      "Step  695 \t training loss: 0.0080718994140625 \t 14:52:45.905808\n",
      "Step  696 \t training loss: 0.00806427001953125 \t 14:52:57.432098\n",
      "Step  697 \t training loss: 0.007213592529296875 \t 14:53:08.709115\n",
      "Step  698 \t training loss: 0.008209228515625 \t 14:53:19.989401\n",
      "Step  699 \t training loss: 0.008941650390625 \t 14:53:31.260906\n",
      "Step  700 \t training loss: 0.007476806640625 \t 14:53:42.546042\n",
      "validation loss: 5.1484375\n",
      "Step  701 \t training loss: 0.0086212158203125 \t 14:53:54.404174\n",
      "Step  702 \t training loss: 0.00676727294921875 \t 14:54:05.695005\n",
      "Step  703 \t training loss: 0.0088958740234375 \t 14:54:17.316154\n",
      "Step  704 \t training loss: 0.00655364990234375 \t 14:54:28.804532\n",
      "Step  705 \t training loss: 0.00701904296875 \t 14:54:40.081795\n",
      "Step  706 \t training loss: 0.0072021484375 \t 14:54:51.341410\n",
      "Step  707 \t training loss: 0.00827789306640625 \t 14:55:02.583021\n",
      "Step  708 \t training loss: 0.006927490234375 \t 14:55:13.858224\n",
      "Step  709 \t training loss: 0.006618499755859375 \t 14:55:25.126017\n",
      "Step  710 \t training loss: 0.00968170166015625 \t 14:55:36.386646\n",
      "Step  711 \t training loss: 0.00803375244140625 \t 14:55:48.027258\n",
      "Step  712 \t training loss: 0.007503509521484375 \t 14:55:59.538512\n",
      "Step  713 \t training loss: 0.006786346435546875 \t 14:56:10.801353\n",
      "Step  714 \t training loss: 0.00841522216796875 \t 14:56:22.082974\n",
      "Step  715 \t training loss: 0.0066986083984375 \t 14:56:33.351141\n",
      "Step  716 \t training loss: 0.006336212158203125 \t 14:56:44.612250\n",
      "Step  717 \t training loss: 0.00969696044921875 \t 14:56:55.890796\n",
      "Step  718 \t training loss: 0.007801055908203125 \t 14:57:07.161948\n",
      "Step  719 \t training loss: 0.005512237548828125 \t 14:57:18.795227\n",
      "Step  720 \t training loss: 0.007236480712890625 \t 14:57:30.303609\n",
      "validation loss: 5.078125\n",
      "**************************************************************************************************** \n",
      "Question:  !Evaluate 0 - 3 - -6 - -2.\"                                                                                                                                                                                                                                     \n",
      "Actual Answer:  !5\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\".\"\"\"\"\"\"\"\"\"\"\"\n",
      "Step  721 \t training loss: 0.00627899169921875 \t 14:57:44.456982\n",
      "Step  722 \t training loss: 0.006565093994140625 \t 14:57:55.725679\n",
      "Step  723 \t training loss: 0.00745391845703125 \t 14:58:06.990870\n",
      "Step  724 \t training loss: 0.00666046142578125 \t 14:58:18.262692\n",
      "Step  725 \t training loss: 0.00789642333984375 \t 14:58:29.514635\n",
      "Step  726 \t training loss: 0.0078125 \t 14:58:40.778625\n",
      "Step  727 \t training loss: 0.005664825439453125 \t 14:58:52.410324\n",
      "Step  728 \t training loss: 0.007099151611328125 \t 14:59:03.930136\n",
      "Step  729 \t training loss: 0.00598907470703125 \t 14:59:15.193966\n",
      "Step  730 \t training loss: 0.006862640380859375 \t 14:59:26.451405\n",
      "Step  731 \t training loss: 0.005374908447265625 \t 14:59:37.717724\n",
      "Step  732 \t training loss: 0.005847930908203125 \t 14:59:48.973461\n",
      "Step  733 \t training loss: 0.0069580078125 \t 15:00:00.241150\n",
      "Step  734 \t training loss: 0.007110595703125 \t 15:00:11.491574\n",
      "Step  735 \t training loss: 0.00699615478515625 \t 15:00:23.117030\n",
      "Step  736 \t training loss: 0.005168914794921875 \t 15:00:34.607700\n",
      "Step  737 \t training loss: 0.007747650146484375 \t 15:00:45.881843\n",
      "Step  738 \t training loss: 0.00634765625 \t 15:00:57.135546\n",
      "Step  739 \t training loss: 0.0059814453125 \t 15:01:08.401644\n",
      "Step  740 \t training loss: 0.005542755126953125 \t 15:01:19.681425\n",
      "validation loss: 5.1875\n",
      "Step  741 \t training loss: 0.007022857666015625 \t 15:01:31.544526\n",
      "Step  742 \t training loss: 0.007808685302734375 \t 15:01:42.804591\n",
      "Step  743 \t training loss: 0.005523681640625 \t 15:01:54.436759\n",
      "Step  744 \t training loss: 0.0061492919921875 \t 15:02:05.962791\n",
      "Step  745 \t training loss: 0.006008148193359375 \t 15:02:17.224656\n",
      "Step  746 \t training loss: 0.005245208740234375 \t 15:02:28.496716\n",
      "Step  747 \t training loss: 0.00665283203125 \t 15:02:39.749735\n",
      "Step  748 \t training loss: 0.00597381591796875 \t 15:02:51.012045\n",
      "Step  749 \t training loss: 0.00836181640625 \t 15:03:02.284178\n",
      "Step  750 \t training loss: 0.006649017333984375 \t 15:03:13.539577\n",
      "Step  751 \t training loss: 0.00754547119140625 \t 15:03:25.164894\n",
      "Step  752 \t training loss: 0.0056304931640625 \t 15:03:36.660602\n",
      "Step  753 \t training loss: 0.0084991455078125 \t 15:03:47.943738\n",
      "Step  754 \t training loss: 0.004528045654296875 \t 15:03:59.212841\n",
      "Step  755 \t training loss: 0.005298614501953125 \t 15:04:10.478432\n",
      "Step  756 \t training loss: 0.006122589111328125 \t 15:04:21.757903\n",
      "Step  757 \t training loss: 0.00669097900390625 \t 15:04:33.108250\n",
      "Step  758 \t training loss: 0.006343841552734375 \t 15:04:44.479340\n",
      "Step  759 \t training loss: 0.00567626953125 \t 15:04:56.305965\n",
      "Step  760 \t training loss: 0.00577545166015625 \t 15:05:07.958821\n",
      "validation loss: 5.43359375\n",
      "Step  761 \t training loss: 0.00659942626953125 \t 15:05:19.897443\n",
      "Step  762 \t training loss: 0.005382537841796875 \t 15:05:31.178391\n",
      "Step  763 \t training loss: 0.005344390869140625 \t 15:05:42.440722\n",
      "Step  764 \t training loss: 0.0040130615234375 \t 15:05:53.722127\n",
      "Step  765 \t training loss: 0.005767822265625 \t 15:06:04.977146\n",
      "Step  766 \t training loss: 0.005435943603515625 \t 15:06:16.247461\n",
      "Step  767 \t training loss: 0.004543304443359375 \t 15:06:27.884658\n",
      "Step  768 \t training loss: 0.005390167236328125 \t 15:06:39.388871\n",
      "Step  769 \t training loss: 0.00592041015625 \t 15:06:50.642701\n",
      "Step  770 \t training loss: 0.003849029541015625 \t 15:07:01.922234\n",
      "Step  771 \t training loss: 0.004730224609375 \t 15:07:13.177928\n",
      "Step  772 \t training loss: 0.004886627197265625 \t 15:07:24.441193\n",
      "Step  773 \t training loss: 0.005130767822265625 \t 15:07:35.717844\n",
      "Step  774 \t training loss: 0.00577545166015625 \t 15:07:46.977504\n",
      "Step  775 \t training loss: 0.005199432373046875 \t 15:07:58.644206\n",
      "Step  776 \t training loss: 0.0054168701171875 \t 15:08:10.173619\n",
      "Step  777 \t training loss: 0.0054779052734375 \t 15:08:21.437515\n",
      "Step  778 \t training loss: 0.006954193115234375 \t 15:08:32.695612\n",
      "Step  779 \t training loss: 0.006305694580078125 \t 15:08:43.964712\n",
      "Step  780 \t training loss: 0.006053924560546875 \t 15:08:55.228074\n",
      "validation loss: 5.40234375\n",
      "**************************************************************************************************** \n",
      "Question:  !Calculate 153 + -0.05.\"                                                                                                                                                                                                                                        \n",
      "Actual Answer:  !152.95\"                                                                                                                                                                                                                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Prediction:  \"\"\"\".\"\"\"\"\".\"\"\"\"\"\"\"\"\"\"\"\"\"\".\"\"\"\"\"\"\n",
      "Step  781 \t training loss: 0.00504302978515625 \t 15:09:09.261357\n",
      "Step  782 \t training loss: 0.005558013916015625 \t 15:09:20.527225\n",
      "Step  783 \t training loss: 0.00508880615234375 \t 15:09:32.170116\n",
      "Step  784 \t training loss: 0.00562286376953125 \t 15:09:43.725359\n",
      "Step  785 \t training loss: 0.00466156005859375 \t 15:09:54.996950\n",
      "Step  786 \t training loss: 0.005573272705078125 \t 15:10:06.267386\n",
      "Step  787 \t training loss: 0.0056304931640625 \t 15:10:17.536219\n",
      "Step  788 \t training loss: 0.0036907196044921875 \t 15:10:28.794582\n",
      "Step  789 \t training loss: 0.003803253173828125 \t 15:10:40.056140\n",
      "Step  790 \t training loss: 0.0062408447265625 \t 15:10:51.321479\n",
      "Step  791 \t training loss: 0.004611968994140625 \t 15:11:02.948583\n",
      "Step  792 \t training loss: 0.00604248046875 \t 15:11:14.454469\n",
      "Step  793 \t training loss: 0.0046539306640625 \t 15:11:25.721663\n",
      "Step  794 \t training loss: 0.006927490234375 \t 15:11:36.988877\n",
      "Step  795 \t training loss: 0.004909515380859375 \t 15:11:48.249774\n",
      "Step  796 \t training loss: 0.004180908203125 \t 15:11:59.508252\n",
      "Step  797 \t training loss: 0.00560760498046875 \t 15:12:10.766629\n",
      "Step  798 \t training loss: 0.00396728515625 \t 15:12:22.047543\n",
      "Step  799 \t training loss: 0.00812530517578125 \t 15:12:33.676142\n",
      "Step  800 \t training loss: 0.003032684326171875 \t 15:12:45.191798\n",
      "validation loss: 5.51953125\n",
      "Step  801 \t training loss: 0.00408935546875 \t 15:12:57.051246\n",
      "Step  802 \t training loss: 0.006542205810546875 \t 15:13:08.325504\n",
      "Step  803 \t training loss: 0.00620269775390625 \t 15:13:19.603282\n",
      "Step  804 \t training loss: 0.003818511962890625 \t 15:13:30.852896\n",
      "Step  805 \t training loss: 0.005130767822265625 \t 15:13:42.130199\n",
      "Step  806 \t training loss: 0.0042877197265625 \t 15:13:53.403287\n",
      "Step  807 \t training loss: 0.005889892578125 \t 15:14:05.037190\n",
      "Step  808 \t training loss: 0.004314422607421875 \t 15:14:16.568083\n",
      "Step  809 \t training loss: 0.00446319580078125 \t 15:14:27.844016\n",
      "Step  810 \t training loss: 0.00696563720703125 \t 15:14:39.097787\n",
      "Step  811 \t training loss: 0.00437164306640625 \t 15:14:50.364446\n",
      "Step  812 \t training loss: 0.004016876220703125 \t 15:15:01.632415\n",
      "Step  813 \t training loss: 0.00405120849609375 \t 15:15:12.904526\n",
      "Step  814 \t training loss: 0.00490570068359375 \t 15:15:24.165272\n",
      "Step  815 \t training loss: 0.004528045654296875 \t 15:15:35.796772\n",
      "Step  816 \t training loss: 0.004222869873046875 \t 15:15:47.294309\n",
      "Step  817 \t training loss: 0.00357818603515625 \t 15:15:58.553628\n",
      "Step  818 \t training loss: 0.0034618377685546875 \t 15:16:09.829496\n",
      "Step  819 \t training loss: 0.004909515380859375 \t 15:16:21.101046\n",
      "Step  820 \t training loss: 0.003887176513671875 \t 15:16:32.373010\n",
      "validation loss: 5.4296875\n",
      "Step  821 \t training loss: 0.0042266845703125 \t 15:16:44.258312\n",
      "Step  822 \t training loss: 0.004505157470703125 \t 15:16:55.525991\n",
      "Step  823 \t training loss: 0.00550079345703125 \t 15:17:07.145806\n",
      "Step  824 \t training loss: 0.003993988037109375 \t 15:17:18.678125\n",
      "Step  825 \t training loss: 0.0050811767578125 \t 15:17:29.958097\n",
      "Step  826 \t training loss: 0.00312042236328125 \t 15:17:41.209213\n",
      "Step  827 \t training loss: 0.0034122467041015625 \t 15:17:52.472580\n",
      "Step  828 \t training loss: 0.004093170166015625 \t 15:18:03.743080\n",
      "Step  829 \t training loss: 0.0042572021484375 \t 15:18:14.982241\n",
      "Step  830 \t training loss: 0.0037384033203125 \t 15:18:26.268360\n",
      "Step  831 \t training loss: 0.0034580230712890625 \t 15:18:37.909947\n",
      "Step  832 \t training loss: 0.004138946533203125 \t 15:18:49.400963\n",
      "Step  833 \t training loss: 0.00405120849609375 \t 15:19:00.664660\n",
      "Step  834 \t training loss: 0.00478363037109375 \t 15:19:11.942982\n",
      "Step  835 \t training loss: 0.00279998779296875 \t 15:19:23.204904\n",
      "Step  836 \t training loss: 0.0038928985595703125 \t 15:19:34.521087\n",
      "Step  837 \t training loss: 0.004817962646484375 \t 15:19:45.898185\n",
      "Step  838 \t training loss: 0.004558563232421875 \t 15:19:57.158747\n",
      "Step  839 \t training loss: 0.004428863525390625 \t 15:20:08.823668\n",
      "Step  840 \t training loss: 0.0036640167236328125 \t 15:20:20.359376\n",
      "validation loss: 5.546875\n",
      "**************************************************************************************************** \n",
      "Question:  !What is -1 + -2 + 5 + 2 + -3?\"                                                                                                                                                                                                                                 \n",
      "Actual Answer:  !1\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  \"\"\"\"\".\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\".\"\"\"\"\"\"\n",
      "Step  841 \t training loss: 0.0037631988525390625 \t 15:20:34.519754\n",
      "Step  842 \t training loss: 0.0036773681640625 \t 15:20:45.639934\n",
      "Step  843 \t training loss: 0.0034389495849609375 \t 15:20:56.813369\n",
      "Step  844 \t training loss: 0.005306243896484375 \t 15:21:07.963123\n",
      "Step  845 \t training loss: 0.003177642822265625 \t 15:21:19.106152\n",
      "Step  846 \t training loss: 0.004734039306640625 \t 15:21:29.936111\n",
      "Step  847 \t training loss: 0.0036163330078125 \t 15:21:41.237020\n",
      "Step  848 \t training loss: 0.0036907196044921875 \t 15:21:52.598585\n",
      "Step  849 \t training loss: 0.003498077392578125 \t 15:22:03.754431\n",
      "Step  850 \t training loss: 0.00266265869140625 \t 15:22:14.773193\n",
      "Step  851 \t training loss: 0.005039215087890625 \t 15:22:25.582057\n",
      "Step  852 \t training loss: 0.004665374755859375 \t 15:22:36.381004\n",
      "Step  853 \t training loss: 0.0044403076171875 \t 15:22:47.187070\n",
      "Step  854 \t training loss: 0.00318145751953125 \t 15:22:58.070900\n",
      "Step  855 \t training loss: 0.005710601806640625 \t 15:23:09.282988\n",
      "Step  856 \t training loss: 0.00379180908203125 \t 15:23:20.321251\n",
      "Step  857 \t training loss: 0.00371551513671875 \t 15:23:31.119194\n",
      "Step  858 \t training loss: 0.00616455078125 \t 15:23:41.921932\n",
      "Step  859 \t training loss: 0.00337982177734375 \t 15:23:52.721682\n",
      "Step  860 \t training loss: 0.0028553009033203125 \t 15:24:03.516205\n",
      "validation loss: 5.57421875\n",
      "Step  861 \t training loss: 0.0032978057861328125 \t 15:24:14.893921\n",
      "Step  862 \t training loss: 0.003200531005859375 \t 15:24:25.697914\n",
      "Step  863 \t training loss: 0.00296783447265625 \t 15:24:36.833913\n",
      "Step  864 \t training loss: 0.003204345703125 \t 15:24:47.882668\n",
      "Step  865 \t training loss: 0.0035381317138671875 \t 15:24:58.678504\n",
      "Step  866 \t training loss: 0.00362396240234375 \t 15:25:09.469363\n",
      "Step  867 \t training loss: 0.003536224365234375 \t 15:25:20.262863\n",
      "Step  868 \t training loss: 0.003326416015625 \t 15:25:31.060188\n",
      "Step  869 \t training loss: 0.0030975341796875 \t 15:25:41.857977\n",
      "Step  870 \t training loss: 0.0037479400634765625 \t 15:25:52.658535\n",
      "Step  871 \t training loss: 0.0032558441162109375 \t 15:26:03.794245\n",
      "Step  872 \t training loss: 0.0038909912109375 \t 15:26:14.817914\n",
      "Step  873 \t training loss: 0.0027675628662109375 \t 15:26:25.662591\n",
      "Step  874 \t training loss: 0.0036563873291015625 \t 15:26:36.462596\n",
      "Step  875 \t training loss: 0.0026264190673828125 \t 15:26:47.259463\n",
      "Step  876 \t training loss: 0.00421142578125 \t 15:26:58.060371\n",
      "Step  877 \t training loss: 0.0023651123046875 \t 15:27:08.854418\n",
      "Step  878 \t training loss: 0.0034618377685546875 \t 15:27:19.648368\n",
      "Step  879 \t training loss: 0.0029926300048828125 \t 15:27:30.782379\n",
      "Step  880 \t training loss: 0.0038509368896484375 \t 15:27:41.810981\n",
      "validation loss: 5.6875\n",
      "Step  881 \t training loss: 0.002857208251953125 \t 15:27:53.191085\n",
      "Step  882 \t training loss: 0.003398895263671875 \t 15:28:04.008411\n",
      "Step  883 \t training loss: 0.0030231475830078125 \t 15:28:14.812558\n",
      "Step  884 \t training loss: 0.00292205810546875 \t 15:28:25.617718\n",
      "Step  885 \t training loss: 0.003147125244140625 \t 15:28:36.415255\n",
      "Step  886 \t training loss: 0.0030612945556640625 \t 15:28:47.217200\n",
      "Step  887 \t training loss: 0.0038471221923828125 \t 15:28:58.348695\n",
      "Step  888 \t training loss: 0.0038623809814453125 \t 15:29:09.399528\n",
      "Step  889 \t training loss: 0.0029697418212890625 \t 15:29:20.196587\n",
      "Step  890 \t training loss: 0.0029735565185546875 \t 15:29:30.993865\n",
      "Step  891 \t training loss: 0.00257110595703125 \t 15:29:41.791773\n",
      "Step  892 \t training loss: 0.003047943115234375 \t 15:29:52.590964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  893 \t training loss: 0.0028667449951171875 \t 15:30:03.397772\n",
      "Step  894 \t training loss: 0.0024242401123046875 \t 15:30:14.199768\n",
      "Step  895 \t training loss: 0.0026397705078125 \t 15:30:25.346225\n",
      "Step  896 \t training loss: 0.003826141357421875 \t 15:30:36.397935\n",
      "Step  897 \t training loss: 0.003276824951171875 \t 15:30:47.206092\n",
      "Step  898 \t training loss: 0.0031337738037109375 \t 15:30:58.006700\n",
      "Step  899 \t training loss: 0.0038242340087890625 \t 15:31:08.831239\n",
      "Step  900 \t training loss: 0.002895355224609375 \t 15:31:19.626965\n",
      "validation loss: 5.76953125\n",
      "**************************************************************************************************** \n",
      "Question:  !Evaluate -29 + 24 - (2 - 9).\"                                                                                                                                                                                                                                  \n",
      "Actual Answer:  !2\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "Step  901 \t training loss: 0.0025959014892578125 \t 15:31:33.197801\n",
      "Step  902 \t training loss: 0.003063201904296875 \t 15:31:43.998478\n",
      "Step  903 \t training loss: 0.0028820037841796875 \t 15:31:55.135013\n",
      "Step  904 \t training loss: 0.0033092498779296875 \t 15:32:06.195215\n",
      "Step  905 \t training loss: 0.004505157470703125 \t 15:32:16.998519\n",
      "Step  906 \t training loss: 0.00250244140625 \t 15:32:27.816487\n",
      "Step  907 \t training loss: 0.002216339111328125 \t 15:32:38.611004\n",
      "Step  908 \t training loss: 0.002895355224609375 \t 15:32:49.411738\n",
      "Step  909 \t training loss: 0.00228118896484375 \t 15:33:00.208278\n",
      "Step  910 \t training loss: 0.0033969879150390625 \t 15:33:11.010155\n",
      "Step  911 \t training loss: 0.002872467041015625 \t 15:33:22.146865\n",
      "Step  912 \t training loss: 0.0027942657470703125 \t 15:33:33.191726\n",
      "Step  913 \t training loss: 0.003910064697265625 \t 15:33:43.983961\n",
      "Step  914 \t training loss: 0.0037746429443359375 \t 15:33:54.775547\n",
      "Step  915 \t training loss: 0.003719329833984375 \t 15:34:05.573960\n",
      "Step  916 \t training loss: 0.0032024383544921875 \t 15:34:16.370025\n",
      "Step  917 \t training loss: 0.0027675628662109375 \t 15:34:27.175332\n",
      "Step  918 \t training loss: 0.0026874542236328125 \t 15:34:37.971139\n",
      "Step  919 \t training loss: 0.002460479736328125 \t 15:34:49.104861\n",
      "Step  920 \t training loss: 0.0027103424072265625 \t 15:35:00.159859\n",
      "validation loss: 5.85546875\n",
      "Step  921 \t training loss: 0.0028591156005859375 \t 15:35:11.552188\n",
      "Step  922 \t training loss: 0.0024509429931640625 \t 15:35:22.357737\n",
      "Step  923 \t training loss: 0.00228118896484375 \t 15:35:33.164993\n",
      "Step  924 \t training loss: 0.0031566619873046875 \t 15:35:43.964439\n",
      "Step  925 \t training loss: 0.00220489501953125 \t 15:35:54.760149\n",
      "Step  926 \t training loss: 0.0030040740966796875 \t 15:36:05.561773\n",
      "Step  927 \t training loss: 0.0031032562255859375 \t 15:36:16.699978\n",
      "Step  928 \t training loss: 0.0035305023193359375 \t 15:36:27.751051\n",
      "Step  929 \t training loss: 0.0028362274169921875 \t 15:36:38.552573\n",
      "Step  930 \t training loss: 0.0023345947265625 \t 15:36:49.352794\n",
      "Step  931 \t training loss: 0.002162933349609375 \t 15:37:00.152166\n",
      "Step  932 \t training loss: 0.0038547515869140625 \t 15:37:10.967474\n",
      "Step  933 \t training loss: 0.0028247833251953125 \t 15:37:21.773669\n",
      "Step  934 \t training loss: 0.0030727386474609375 \t 15:37:32.573272\n",
      "Step  935 \t training loss: 0.002666473388671875 \t 15:37:43.721664\n",
      "Step  936 \t training loss: 0.0021610260009765625 \t 15:37:54.760732\n",
      "Step  937 \t training loss: 0.0022258758544921875 \t 15:38:05.557488\n",
      "Step  938 \t training loss: 0.0028476715087890625 \t 15:38:16.354645\n",
      "Step  939 \t training loss: 0.0024356842041015625 \t 15:38:27.157151\n",
      "Step  940 \t training loss: 0.0026111602783203125 \t 15:38:37.950663\n",
      "validation loss: 5.921875\n",
      "Step  941 \t training loss: 0.0024566650390625 \t 15:38:49.441107\n",
      "Step  942 \t training loss: 0.003787994384765625 \t 15:39:00.519546\n",
      "Step  943 \t training loss: 0.0018892288208007812 \t 15:39:11.646106\n",
      "Step  944 \t training loss: 0.0028591156005859375 \t 15:39:22.669193\n",
      "Step  945 \t training loss: 0.002208709716796875 \t 15:39:33.459152\n",
      "Step  946 \t training loss: 0.0022945404052734375 \t 15:39:44.245037\n",
      "Step  947 \t training loss: 0.0023899078369140625 \t 15:39:55.039869\n",
      "Step  948 \t training loss: 0.0020809173583984375 \t 15:40:05.824163\n",
      "Step  949 \t training loss: 0.0022220611572265625 \t 15:40:16.604835\n",
      "Step  950 \t training loss: 0.0026988983154296875 \t 15:40:27.397097\n",
      "Step  951 \t training loss: 0.0022945404052734375 \t 15:40:38.521244\n",
      "Step  952 \t training loss: 0.0036258697509765625 \t 15:40:49.564564\n",
      "Step  953 \t training loss: 0.0030918121337890625 \t 15:41:00.345853\n",
      "Step  954 \t training loss: 0.0024776458740234375 \t 15:41:11.130391\n",
      "Step  955 \t training loss: 0.0018405914306640625 \t 15:41:21.913075\n",
      "Step  956 \t training loss: 0.0028743743896484375 \t 15:41:32.696705\n",
      "Step  957 \t training loss: 0.0019073486328125 \t 15:41:43.481197\n",
      "Step  958 \t training loss: 0.0026111602783203125 \t 15:41:54.267081\n",
      "Step  959 \t training loss: 0.0024261474609375 \t 15:42:05.393561\n",
      "Step  960 \t training loss: 0.0017414093017578125 \t 15:42:16.442833\n",
      "validation loss: 5.90234375\n",
      "**************************************************************************************************** \n",
      "Question:  !What is -0.51 - -214?\"                                                                                                                                                                                                                                         \n",
      "Actual Answer:  !213.49\"                                                                                                                                                                                                                                                        \n",
      "Decoded Prediction:  .\"\"\"\"\"\".\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\".\n",
      "Step  961 \t training loss: 0.002246856689453125 \t 15:42:30.089443\n",
      "Step  962 \t training loss: 0.002658843994140625 \t 15:42:40.876910\n",
      "Step  963 \t training loss: 0.0028858184814453125 \t 15:42:51.666781\n",
      "Step  964 \t training loss: 0.0022907257080078125 \t 15:43:02.458267\n",
      "Step  965 \t training loss: 0.0018157958984375 \t 15:43:13.247439\n",
      "Step  966 \t training loss: 0.0020885467529296875 \t 15:43:24.042508\n",
      "Step  967 \t training loss: 0.002410888671875 \t 15:43:35.179531\n",
      "Step  968 \t training loss: 0.0017690658569335938 \t 15:43:46.213726\n",
      "Step  969 \t training loss: 0.001834869384765625 \t 15:43:57.001502\n",
      "Step  970 \t training loss: 0.0024051666259765625 \t 15:44:07.796364\n",
      "Step  971 \t training loss: 0.0017213821411132812 \t 15:44:18.584926\n",
      "Step  972 \t training loss: 0.001922607421875 \t 15:44:29.388988\n",
      "Step  973 \t training loss: 0.0022430419921875 \t 15:44:40.173886\n",
      "Step  974 \t training loss: 0.0022735595703125 \t 15:44:50.970780\n",
      "Step  975 \t training loss: 0.0019159317016601562 \t 15:45:02.128323\n",
      "Step  976 \t training loss: 0.0016584396362304688 \t 15:45:13.152870\n",
      "Step  977 \t training loss: 0.0020751953125 \t 15:45:23.939720\n",
      "Step  978 \t training loss: 0.002330780029296875 \t 15:45:34.721498\n",
      "Step  979 \t training loss: 0.0027217864990234375 \t 15:45:45.516849\n",
      "Step  980 \t training loss: 0.0019464492797851562 \t 15:45:56.302560\n",
      "validation loss: 5.98828125\n",
      "Step  981 \t training loss: 0.002147674560546875 \t 15:46:07.658991\n",
      "Step  982 \t training loss: 0.0016632080078125 \t 15:46:18.444594\n",
      "Step  983 \t training loss: 0.00218963623046875 \t 15:46:29.568728\n",
      "Step  984 \t training loss: 0.002197265625 \t 15:46:40.612017\n",
      "Step  985 \t training loss: 0.0018157958984375 \t 15:46:51.395325\n",
      "Step  986 \t training loss: 0.001811981201171875 \t 15:47:02.180857\n",
      "Step  987 \t training loss: 0.0023860931396484375 \t 15:47:12.962967\n",
      "Step  988 \t training loss: 0.0019588470458984375 \t 15:47:23.752057\n",
      "Step  989 \t training loss: 0.0015478134155273438 \t 15:47:34.535072\n",
      "Step  990 \t training loss: 0.0016336441040039062 \t 15:47:45.325226\n",
      "Step  991 \t training loss: 0.0013761520385742188 \t 15:47:56.447631\n",
      "Step  992 \t training loss: 0.0018835067749023438 \t 15:48:07.462896\n",
      "Step  993 \t training loss: 0.002109527587890625 \t 15:48:18.260697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  994 \t training loss: 0.0020122528076171875 \t 15:48:29.049950\n",
      "Step  995 \t training loss: 0.0021266937255859375 \t 15:48:39.839285\n",
      "Step  996 \t training loss: 0.0014696121215820312 \t 15:48:50.614756\n",
      "Step  997 \t training loss: 0.0013275146484375 \t 15:49:01.404205\n",
      "Step  998 \t training loss: 0.0013513565063476562 \t 15:49:12.194648\n",
      "Step  999 \t training loss: 0.0015239715576171875 \t 15:49:23.316055\n",
      "Step  1000 \t training loss: 0.0016222000122070312 \t 15:49:34.346446\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "validation loss: 6.01953125\n",
      "Step  1001 \t training loss: 0.00188446044921875 \t 15:49:45.699851\n",
      "Step  1002 \t training loss: 0.0015239715576171875 \t 15:49:56.491734\n",
      "Step  1003 \t training loss: 0.0020599365234375 \t 15:50:07.279901\n",
      "Step  1004 \t training loss: 0.00167083740234375 \t 15:50:18.184677\n",
      "Step  1005 \t training loss: 0.0017566680908203125 \t 15:50:29.069014\n",
      "Step  1006 \t training loss: 0.0016260147094726562 \t 15:50:39.946385\n",
      "Step  1007 \t training loss: 0.0019350051879882812 \t 15:50:51.708011\n",
      "Step  1008 \t training loss: 0.001766204833984375 \t 15:51:03.293224\n",
      "Step  1009 \t training loss: 0.0017070770263671875 \t 15:51:14.202079\n",
      "Step  1010 \t training loss: 0.0012969970703125 \t 15:51:25.026462\n",
      "Step  1011 \t training loss: 0.00217437744140625 \t 15:51:35.849255\n",
      "Step  1012 \t training loss: 0.0018796920776367188 \t 15:51:46.673650\n",
      "Step  1013 \t training loss: 0.0014209747314453125 \t 15:51:57.491987\n",
      "Step  1014 \t training loss: 0.0020275115966796875 \t 15:52:08.311091\n",
      "Step  1015 \t training loss: 0.0019588470458984375 \t 15:52:19.474782\n",
      "Step  1016 \t training loss: 0.00218963623046875 \t 15:52:30.549304\n",
      "Step  1017 \t training loss: 0.0015821456909179688 \t 15:52:41.383171\n",
      "Step  1018 \t training loss: 0.0030155181884765625 \t 15:52:52.234680\n",
      "Step  1019 \t training loss: 0.0016326904296875 \t 15:53:03.067790\n",
      "Step  1020 \t training loss: 0.0014858245849609375 \t 15:53:13.895729\n",
      "validation loss: 6.12109375\n",
      "**************************************************************************************************** \n",
      "Question:  !What is -0.31 less than -16.9?\"                                                                                                                                                                                                                                \n",
      "Actual Answer:  !-16.59\"                                                                                                                                                                                                                                                        \n",
      "Decoded Prediction:  .\"\"\".\"\"\"\".8\"\".\".\"\".\"\"\"\"\"\"\".\"\"f.\"\n",
      "Step  1021 \t training loss: 0.0022716522216796875 \t 15:53:27.449268\n",
      "Step  1022 \t training loss: 0.0017547607421875 \t 15:53:38.272885\n",
      "Step  1023 \t training loss: 0.0013113021850585938 \t 15:53:49.424952\n",
      "Step  1024 \t training loss: 0.00165557861328125 \t 15:54:00.473249\n",
      "Step  1025 \t training loss: 0.0018301010131835938 \t 15:54:11.289280\n",
      "Step  1026 \t training loss: 0.0011034011840820312 \t 15:54:22.117742\n",
      "Step  1027 \t training loss: 0.0021152496337890625 \t 15:54:32.935291\n",
      "Step  1028 \t training loss: 0.0017490386962890625 \t 15:54:43.758208\n",
      "Step  1029 \t training loss: 0.0011205673217773438 \t 15:54:54.575057\n",
      "Step  1030 \t training loss: 0.0016918182373046875 \t 15:55:05.938613\n",
      "Step  1031 \t training loss: 0.001407623291015625 \t 15:55:17.467553\n",
      "Step  1032 \t training loss: 0.0018339157104492188 \t 15:55:28.988684\n",
      "Step  1033 \t training loss: 0.0016002655029296875 \t 15:55:40.455831\n",
      "Step  1034 \t training loss: 0.0013380050659179688 \t 15:55:51.998275\n",
      "Step  1035 \t training loss: 0.0018672943115234375 \t 15:56:03.291175\n",
      "Step  1036 \t training loss: 0.00140380859375 \t 15:56:14.647798\n",
      "Step  1037 \t training loss: 0.0015325546264648438 \t 15:56:26.118876\n",
      "Step  1038 \t training loss: 0.0016345977783203125 \t 15:56:37.307050\n",
      "Step  1039 \t training loss: 0.0018520355224609375 \t 15:56:48.785069\n",
      "Step  1040 \t training loss: 0.0024566650390625 \t 15:57:00.154484\n",
      "validation loss: 6.15625\n",
      "Step  1041 \t training loss: 0.0013360977172851562 \t 15:57:11.814518\n",
      "Step  1042 \t training loss: 0.0017442703247070312 \t 15:57:23.021544\n",
      "Step  1043 \t training loss: 0.0015554428100585938 \t 15:57:33.872853\n",
      "Step  1044 \t training loss: 0.0015954971313476562 \t 15:57:45.046865\n",
      "Step  1045 \t training loss: 0.0016183853149414062 \t 15:57:56.154669\n",
      "Step  1046 \t training loss: 0.0014123916625976562 \t 15:58:07.285089\n",
      "Step  1047 \t training loss: 0.0021915435791015625 \t 15:58:18.821580\n",
      "Step  1048 \t training loss: 0.0012845993041992188 \t 15:58:29.904363\n",
      "Step  1049 \t training loss: 0.0014696121215820312 \t 15:58:40.769565\n",
      "Step  1050 \t training loss: 0.0026645660400390625 \t 15:58:51.659681\n",
      "Step  1051 \t training loss: 0.0016088485717773438 \t 15:59:02.615364\n",
      "Step  1052 \t training loss: 0.0016326904296875 \t 15:59:13.436203\n",
      "Step  1053 \t training loss: 0.0021610260009765625 \t 15:59:24.470523\n",
      "Step  1054 \t training loss: 0.0014057159423828125 \t 15:59:35.962769\n",
      "Step  1055 \t training loss: 0.0011434555053710938 \t 15:59:47.724137\n",
      "Step  1056 \t training loss: 0.0021114349365234375 \t 15:59:58.911358\n",
      "Step  1057 \t training loss: 0.0011720657348632812 \t 16:00:09.840356\n",
      "Step  1058 \t training loss: 0.001430511474609375 \t 16:00:20.861551\n",
      "Step  1059 \t training loss: 0.001392364501953125 \t 16:00:31.936282\n",
      "Step  1060 \t training loss: 0.0017719268798828125 \t 16:00:42.761253\n",
      "validation loss: 6.2578125\n",
      "Step  1061 \t training loss: 0.0012531280517578125 \t 16:00:54.217107\n",
      "Step  1062 \t training loss: 0.001129150390625 \t 16:01:05.034337\n",
      "Step  1063 \t training loss: 0.0014314651489257812 \t 16:01:16.124221\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f44ed270c31c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_qs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_qs_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "i = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "# for batch_idx, batch in enumerate(tqdm(train_loader, mininterval=2, leave=False)):\n",
    "    batch_qs, batch_qs_mask, batch_as, batch_as_mask = map(lambda x: x.to(device), next(train_loader))\n",
    "    # exclude the 0th element as it is BOS\n",
    "    gold_as = batch_as[:, 1:]\n",
    "    \n",
    "    if (i % GENERATE_EVERY) - 1 == 0:\n",
    "        enc_dec.eval()\n",
    "        gen_qs, gen_qs_mask, gen_as, gen_as_mask = next(gen_loader)\n",
    "    #         inp = random.choice(val_ds)[:-1]\n",
    "        prime = np_decode_string(gen_qs.numpy())\n",
    "        print('*' * 100, \"\\nQuestion: \", prime)\n",
    "        print(\"Actual Answer: \", np_decode_string(gen_as.numpy()))\n",
    "    #     print(\"Raw Answer: \", gen_as.numpy())\n",
    "        gen_qs = gen_qs.to(device)\n",
    "        gen_as = gen_as.to(device)\n",
    "        gen_qs_mask = gen_qs_mask.to(device)\n",
    "        sample = enc_dec.generate(gen_qs, gen_as, GENERATE_LENGTH, enc_input_mask = gen_qs_mask)\n",
    "        sample = sample.cpu().numpy()\n",
    "        output_str = np_decode_string(sample)\n",
    "    #     print(\"Raw Prediction: \", sample)\n",
    "        print(\"Decoded Prediction: \", output_str)\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-train_loss.txt\", train_loss_list)\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\", val_loss_list)\n",
    "        \n",
    "#         with open(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-train_loss.txt\", \"w\") as fp:\n",
    "#             pickle.dumps(train_loss_list, fp)\n",
    "#         with open(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\", \"w\") as fp:\n",
    "#             pickle.dumps(val_loss_list, fp)\n",
    "#         print(\"Logs saved to \", \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\")\n",
    "            \n",
    "            \n",
    "\n",
    "    enc_dec.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        train_loss = enc_dec(batch_qs, batch_as, return_loss = True, enc_input_mask = batch_qs_mask)\n",
    "        with amp.scale_loss(train_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "\n",
    "    \n",
    "#     if batch_idx % GRADIENT_ACCUMULATE_EVERY == 0:\n",
    "    print(\"Step \", i, \"\\t\", f'training loss: {train_loss.item()}', \"\\t\", datetime.now().time() )\n",
    "    train_loss_list.append((i, train_loss.item()))\n",
    "    torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 0.1)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        val_batch_qs, val_batch_qs_mask, val_batch_as, val_batch_as_mask = map(lambda x: x.to(device), next(val_loader))\n",
    "        enc_dec.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = enc_dec(val_batch_qs, val_batch_as, return_loss = True, enc_input_mask = val_batch_qs_mask)\n",
    "            print(f'validation loss: {val_loss.item()}')\n",
    "            val_loss_list.append((i, val_loss.item()))\n",
    "\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([train_loss_list[1]])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
