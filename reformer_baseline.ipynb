{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# code based off of \n",
    "# https://github.com/mandubian/pytorch_math_dataset and\n",
    "# https://github.com/lucidrains/reformer-pytorch\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "from apex import amp\n",
    "import pickle\n",
    "\n",
    "\n",
    "import mandubian.math_dataset\n",
    "from mandubian.math_dataset import MathDatasetManager\n",
    "from mandubian.transformer import Constants\n",
    "\n",
    "# from transformer.Models import Transformer\n",
    "from mandubian.math_dataset import (\n",
    "    random_split_dataset,\n",
    "    question_answer_to_mask_batch_collate_fn\n",
    ")\n",
    "from mandubian.math_dataset import np_encode_string, np_decode_string\n",
    "import mandubian.model_process\n",
    "import mandubian.utils\n",
    "from mandubian.tensorboard_utils import Tensorboard\n",
    "from mandubian.tensorboard_utils import tensorboard_event_accumulator\n",
    "\n",
    "import mandubian.checkpoints\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 detected CUDA devices\n",
      "Using CUDA device:  0\n",
      "GeForce RTX 2080\n",
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "print(torch.cuda.device_count(), \"detected CUDA devices\")\n",
    "cuda_device = torch.cuda.current_device()\n",
    "print(\"Using CUDA device: \", cuda_device)\n",
    "print(torch.cuda.get_device_name(cuda_device))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucidrains_reformer.reformer_pytorch import ReformerLM, Autopadder, Recorder\n",
    "from lucidrains_reformer.reformer_pytorch import ReformerEncDec\n",
    "from lucidrains_reformer.reformer_pytorch.generative_tools import TrainingWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Math Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized MultiFilesMathDataset with categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus'] and types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "mdsmgr structure ['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_datasets_from_category', 'build_dataset_from_categories', 'build_dataset_from_category', 'build_dataset_from_module', 'build_dataset_from_modules', 'dfs', 'dirs', 'get_categories', 'get_modules_for_category', 'get_types', 'root_dir']\n"
     ]
    }
   ],
   "source": [
    "mdsmgr = MathDatasetManager(\n",
    "  \"/home/jonathan/Repos/final_year_at_ic/awesome_project/mathematics_dataset-v1.0/\"\n",
    ")\n",
    "# Examine dataset structure\n",
    "print(\"mdsmgr structure\", dir(mdsmgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MathDatasetManager._build_datasets_from_category of <mandubian.math_dataset.MathDatasetManager object at 0x7efb77bc1d68>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(MathDatasetManager.__dir__\n",
    "mdsmgr._build_datasets_from_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availables types, problem categories and problem subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus']\n",
      "modules of arithmetic dict_keys(['div', 'nearest_integer_root', 'mul_div_multiple', 'mul', 'add_or_sub', 'add_sub_multiple', 'mixed', 'add_or_sub_in_base', 'simplify_surd', 'add_or_sub_big', 'add_sub_multiple_longer', 'mixed_longer', 'div_big', 'mul_div_multiple_longer', 'mul_big'])\n"
     ]
    }
   ],
   "source": [
    "print(\"types\", list(mdsmgr.get_types()))\n",
    "print(\"categories\", list(mdsmgr.get_categories()))\n",
    "print(\"modules of arithmetic\", mdsmgr.get_modules_for_category('arithmetic'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to manipulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# # Build Dataset from a single module in a category\n",
    "ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy')\n",
    "print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from a single module in a category with limited number of elements\n",
    "# ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy', max_elements=1000)\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from several modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'train-easy')\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'interpolate')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in several categories\n",
    "# ds = mdsmgr.build_dataset_from_categories(['arithmetic', 'polynomials'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"overfit_one_batch_142\"\n",
    "now = datetime.now()\n",
    "unique_id = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "base_dir = \"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tests/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mandubian.math_dataset import (\n",
    "    VOCAB_SZ, MAX_QUESTION_SZ, MAX_ANSWER_SZ\n",
    ")\n",
    "\n",
    "NUM_CPU_THREADS = 12\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 20\n",
    "GENERATE_EVERY  = 60\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# hyperparameters need updates\n",
    "\n",
    "Q_SEQ_LEN = 256\n",
    "A_SEQ_LEN = 30 # unused due to requirements of axial_positon_shape\n",
    "NUM_TOKENS = VOCAB_SZ + 1\n",
    "D_MODEL = 512\n",
    "EMB_DIM = D_MODEL\n",
    "NUM_HEADS = 8\n",
    "QKV_DIM = D_MODEL / NUM_HEADS\n",
    "NUM_LAYERS = 6\n",
    "D_FF = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "# training_data = mdsmgr.build_dataset_from_category('arithmetic','train-easy') # for now\n",
    "training_data = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy', max_elements = 142)\n",
    "\n",
    "# testing data\n",
    "# testing_data_interpolate = mdsmgr.build_dataset_from_category('arithmetic','interpolate')\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_category('arithmetic','extrapolate')\n",
    "\n",
    "testing_data_interpolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'interpolate', max_elements = 1024)\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'extrapolate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lucidrains_reformer.examples.enwik8_simple.train\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "def get_non_pad_mask(seq):\n",
    "    # returns true when token is not PAD and false otherwise\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(Constants.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "# get data splits\n",
    "train_ds, val_ds = mandubian.math_dataset.random_split_dataset(training_data,split_rate=0.9)\n",
    "\n",
    "# get pytorch dataloaders\n",
    "# Questions are padded in question_answer_to_position_batch_collate_fn\n",
    "train_loader = data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "train_loader = cycle(train_loader)\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "val_loader = cycle(val_loader)\n",
    "\n",
    "# for viewing output sequences\n",
    "gen_loader = data.DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "gen_loader = cycle(gen_loader)\n",
    "\n",
    "interpolate_loader = data.DataLoader(\n",
    "    testing_data_interpolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "interpolate_loader = cycle(interpolate_loader)\n",
    "\n",
    "# extrapolate_loader = data.DataLoader(\n",
    "#     testing_data_extrapolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "#     collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "# extrapolate_loader = cycle(extrapolate_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncDec(\n",
       "  (enc): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x4x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x64x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x2x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): Linear(in_features=512, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "enc_dec = ReformerEncDec(\n",
    "    dim = D_MODEL,\n",
    "    enc_num_tokens = NUM_TOKENS,\n",
    "    enc_depth = NUM_LAYERS,\n",
    "    enc_max_seq_len = Q_SEQ_LEN,\n",
    "    dec_num_tokens = NUM_TOKENS,\n",
    "    dec_depth = NUM_LAYERS,\n",
    "    dec_max_seq_len = Q_SEQ_LEN,\n",
    "    # heads = 8 by default\n",
    "    axial_position_shape = (64, 16),  # the shape must multiply up to the max_seq_len (128 x 64 = 8192)\n",
    "    axial_position_dims = (256,256),   # the dims must sum up to the model dimensions (512 + 512 = 1024)\n",
    "    pad_value = Constants.PAD,\n",
    "    ignore_index = Constants.PAD # see if this works. pad_value and ignore_index are probably different\n",
    ").cuda()\n",
    "\n",
    "# enc_dec = Recorder(enc_dec)\n",
    "enc_dec.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer learning rate scheduler, mixed precision setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(enc_dec.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.995), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=4, verbose=True)\n",
    "\n",
    "# mixed precision\n",
    "enc_dec, optimizer = amp.initialize(enc_dec, optimizer, opt_level='O2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 \t training loss: 4.87890625 \t 05:28:53.376316\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "validation loss: 4.9609375\n",
      "**************************************************************************************************** \n",
      "Question:  !What is the value of (10 - 5) + (-6 - -3 - 1)?\"                                                                                                                                                                                                                \n",
      "Actual Answer:  !1\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  111.!.pNS<g 3+ 3...!N.TLp.n ..n.\n",
      "Step  1 \t training loss: 4.93359375 \t 05:29:07.724775\n",
      "Step  2 \t training loss: 3.71484375 \t 05:29:18.968633\n",
      "Step  3 \t training loss: 3.33203125 \t 05:29:30.195391\n",
      "Step  4 \t training loss: 2.904296875 \t 05:29:41.433804\n",
      "Step  5 \t training loss: 2.71484375 \t 05:29:52.715288\n",
      "Step  6 \t training loss: 2.556640625 \t 05:30:03.986727\n",
      "Step  7 \t training loss: 2.5234375 \t 05:30:15.683682\n",
      "Step  8 \t training loss: 2.361328125 \t 05:30:27.380201\n",
      "Step  9 \t training loss: 2.318359375 \t 05:30:38.848941\n",
      "Step  10 \t training loss: 2.279296875 \t 05:30:50.170032\n",
      "Step  11 \t training loss: 2.04296875 \t 05:31:01.468692\n",
      "Step  12 \t training loss: 2.33984375 \t 05:31:12.744459\n",
      "Step  13 \t training loss: 2.275390625 \t 05:31:24.004899\n",
      "Step  14 \t training loss: 2.16015625 \t 05:31:35.305252\n",
      "Step  15 \t training loss: 2.09375 \t 05:31:46.948316\n",
      "Step  16 \t training loss: 2.07421875 \t 05:31:58.571937\n",
      "Step  17 \t training loss: 2.08984375 \t 05:32:09.850931\n",
      "Step  18 \t training loss: 2.255859375 \t 05:32:21.142249\n",
      "Step  19 \t training loss: 2.01953125 \t 05:32:32.408275\n",
      "Step  20 \t training loss: 2.01953125 \t 05:32:43.696061\n",
      "validation loss: 2.154296875\n",
      "Step  21 \t training loss: 2.009765625 \t 05:32:55.711579\n",
      "Step  22 \t training loss: 2.0078125 \t 05:33:06.993538\n",
      "Step  23 \t training loss: 2.10546875 \t 05:33:18.629730\n",
      "Step  24 \t training loss: 2.05859375 \t 05:33:30.261887\n",
      "Step  25 \t training loss: 2.068359375 \t 05:33:41.543411\n",
      "Step  26 \t training loss: 2.111328125 \t 05:33:52.832494\n",
      "Step  27 \t training loss: 1.8701171875 \t 05:34:04.118973\n",
      "Step  28 \t training loss: 1.900390625 \t 05:34:15.412335\n",
      "Step  29 \t training loss: 1.90234375 \t 05:34:26.706573\n",
      "Step  30 \t training loss: 1.7880859375 \t 05:34:37.982630\n",
      "Step  31 \t training loss: 1.8515625 \t 05:34:49.624479\n",
      "Step  32 \t training loss: 1.8798828125 \t 05:35:01.266745\n",
      "Step  33 \t training loss: 1.9326171875 \t 05:35:12.544342\n",
      "Step  34 \t training loss: 1.8427734375 \t 05:35:23.808742\n",
      "Step  35 \t training loss: 1.8779296875 \t 05:35:35.110924\n",
      "Step  36 \t training loss: 1.8994140625 \t 05:35:46.386353\n",
      "Step  37 \t training loss: 1.7705078125 \t 05:35:57.670898\n",
      "Step  38 \t training loss: 1.8310546875 \t 05:36:08.954588\n",
      "Step  39 \t training loss: 1.7900390625 \t 05:36:20.585020\n",
      "Step  40 \t training loss: 1.67578125 \t 05:36:32.243727\n",
      "validation loss: 1.98828125\n",
      "Step  41 \t training loss: 1.8173828125 \t 05:36:44.256029\n",
      "Step  42 \t training loss: 1.8779296875 \t 05:36:55.551160\n",
      "Step  43 \t training loss: 1.8564453125 \t 05:37:06.821234\n",
      "Step  44 \t training loss: 1.8701171875 \t 05:37:18.099318\n",
      "Step  45 \t training loss: 1.8349609375 \t 05:37:29.386727\n",
      "Step  46 \t training loss: 1.908203125 \t 05:37:40.658660\n",
      "Step  47 \t training loss: 1.73828125 \t 05:37:52.289722\n",
      "Step  48 \t training loss: 1.7705078125 \t 05:38:03.928740\n",
      "Step  49 \t training loss: 1.87109375 \t 05:38:15.222440\n",
      "Step  50 \t training loss: 1.80859375 \t 05:38:26.506057\n",
      "Step  51 \t training loss: 1.80859375 \t 05:38:37.792850\n",
      "Step  52 \t training loss: 1.7705078125 \t 05:38:49.087684\n",
      "Step  53 \t training loss: 1.9296875 \t 05:39:00.356849\n",
      "Step  54 \t training loss: 1.7060546875 \t 05:39:11.640550\n",
      "Step  55 \t training loss: 1.73828125 \t 05:39:23.280728\n",
      "Step  56 \t training loss: 1.791015625 \t 05:39:34.965297\n",
      "Step  57 \t training loss: 1.7451171875 \t 05:39:46.248475\n",
      "Step  58 \t training loss: 1.646484375 \t 05:39:57.542928\n",
      "Step  59 \t training loss: 1.70703125 \t 05:40:08.822051\n",
      "Step  60 \t training loss: 1.8408203125 \t 05:40:20.098769\n",
      "validation loss: 1.9453125\n",
      "**************************************************************************************************** \n",
      "Question:  !-4 + (6 - 11) + 4\"                                                                                                                                                                                                                                             \n",
      "Actual Answer:  !-5\"                                                                                                                                                                                                                                                            \n",
      "Decoded Prediction:  48.5\"fmTT1\"81\" \"88\"\"\"f 1\"3\" .38.\n",
      "Step  61 \t training loss: 1.73046875 \t 05:40:34.308101\n",
      "Step  62 \t training loss: 1.787109375 \t 05:40:45.600440\n",
      "Step  63 \t training loss: 1.7421875 \t 05:40:57.232635\n",
      "Step  64 \t training loss: 1.689453125 \t 05:41:08.871740\n",
      "Step  65 \t training loss: 1.7314453125 \t 05:41:20.152805\n",
      "Step  66 \t training loss: 1.7265625 \t 05:41:31.443621\n",
      "Step  67 \t training loss: 1.525390625 \t 05:41:42.708571\n",
      "Step  68 \t training loss: 1.6435546875 \t 05:41:54.025606\n",
      "Step  69 \t training loss: 1.701171875 \t 05:42:05.326589\n",
      "Step  70 \t training loss: 1.7802734375 \t 05:42:16.634485\n",
      "Step  71 \t training loss: 1.736328125 \t 05:42:28.283462\n",
      "Step  72 \t training loss: 1.662109375 \t 05:42:39.979415\n",
      "Step  73 \t training loss: 1.4462890625 \t 05:42:51.359642\n",
      "Step  74 \t training loss: 1.650390625 \t 05:43:02.678530\n",
      "Step  75 \t training loss: 1.6552734375 \t 05:43:14.054489\n",
      "Step  76 \t training loss: 1.7470703125 \t 05:43:25.349023\n",
      "Step  77 \t training loss: 1.6669921875 \t 05:43:36.356726\n",
      "Step  78 \t training loss: 1.6640625 \t 05:43:47.591249\n",
      "Step  79 \t training loss: 1.6787109375 \t 05:43:59.160925\n",
      "Step  80 \t training loss: 1.6669921875 \t 05:44:10.743487\n",
      "validation loss: 1.931640625\n",
      "Step  81 \t training loss: 1.478515625 \t 05:44:22.709472\n",
      "Step  82 \t training loss: 1.640625 \t 05:44:33.933402\n",
      "Step  83 \t training loss: 1.5673828125 \t 05:44:45.165019\n",
      "Step  84 \t training loss: 1.7265625 \t 05:44:56.453837\n",
      "Step  85 \t training loss: 1.7265625 \t 05:45:07.679022\n",
      "Step  86 \t training loss: 1.572265625 \t 05:45:18.910585\n",
      "Step  87 \t training loss: 1.6630859375 \t 05:45:30.483136\n",
      "Step  88 \t training loss: 1.650390625 \t 05:45:42.067580\n",
      "Step  89 \t training loss: 1.634765625 \t 05:45:53.311588\n",
      "Step  90 \t training loss: 1.5537109375 \t 05:46:04.539443\n",
      "Step  91 \t training loss: 1.5263671875 \t 05:46:15.764875\n",
      "Step  92 \t training loss: 1.5546875 \t 05:46:27.010669\n",
      "Step  93 \t training loss: 1.607421875 \t 05:46:38.234437\n",
      "Step  94 \t training loss: 1.55078125 \t 05:46:49.459943\n",
      "Step  95 \t training loss: 1.6796875 \t 05:47:01.028104\n",
      "Step  96 \t training loss: 1.39453125 \t 05:47:12.612167\n",
      "Step  97 \t training loss: 1.619140625 \t 05:47:23.819574\n",
      "Step  98 \t training loss: 1.6171875 \t 05:47:35.053902\n",
      "Step  99 \t training loss: 1.6142578125 \t 05:47:46.275612\n",
      "Step  100 \t training loss: 1.572265625 \t 05:47:57.516306\n",
      "validation loss: 1.9013671875\n",
      "Step  101 \t training loss: 1.568359375 \t 05:48:09.468444\n",
      "Step  102 \t training loss: 1.5556640625 \t 05:48:20.686694\n",
      "Step  103 \t training loss: 1.5283203125 \t 05:48:32.261036\n",
      "Step  104 \t training loss: 1.46484375 \t 05:48:43.857668\n",
      "Step  105 \t training loss: 1.587890625 \t 05:48:55.091040\n",
      "Step  106 \t training loss: 1.630859375 \t 05:49:06.304486\n",
      "Step  107 \t training loss: 1.416015625 \t 05:49:17.535592\n",
      "Step  108 \t training loss: 1.6083984375 \t 05:49:28.760368\n",
      "Step  109 \t training loss: 1.56640625 \t 05:49:39.979134\n",
      "Step  110 \t training loss: 1.5107421875 \t 05:49:51.212905\n",
      "Step  111 \t training loss: 1.560546875 \t 05:50:02.785719\n",
      "Step  112 \t training loss: 1.3779296875 \t 05:50:14.385489\n",
      "Step  113 \t training loss: 1.498046875 \t 05:50:25.607335\n",
      "Step  114 \t training loss: 1.5556640625 \t 05:50:36.860333\n",
      "Step  115 \t training loss: 1.517578125 \t 05:50:48.087645\n",
      "Step  116 \t training loss: 1.51171875 \t 05:50:59.318824\n",
      "Step  117 \t training loss: 1.60546875 \t 05:51:10.528417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  118 \t training loss: 1.490234375 \t 05:51:21.752094\n",
      "Step  119 \t training loss: 1.5341796875 \t 05:51:33.338483\n",
      "Step  120 \t training loss: 1.5263671875 \t 05:51:44.919918\n",
      "validation loss: 1.900390625\n",
      "**************************************************************************************************** \n",
      "Question:  !Total of -1.7 and -8658.\"                                                                                                                                                                                                                                      \n",
      "Actual Answer:  !-8659.7\"                                                                                                                                                                                                                                                       \n",
      "Decoded Prediction:  .7\"1\" \"\" 3\"8\"\"8\"1\" 8\"\"\"@\"\"1\"\".4\"\n",
      "Step  121 \t training loss: 1.5166015625 \t 05:51:59.005346\n",
      "Step  122 \t training loss: 1.4052734375 \t 05:52:10.215838\n",
      "Step  123 \t training loss: 1.4970703125 \t 05:52:21.448787\n",
      "Step  124 \t training loss: 1.466796875 \t 05:52:32.663716\n",
      "Step  125 \t training loss: 1.427734375 \t 05:52:43.865784\n",
      "Step  126 \t training loss: 1.4921875 \t 05:52:55.106116\n",
      "Step  127 \t training loss: 1.4169921875 \t 05:53:06.676101\n",
      "Step  128 \t training loss: 1.4326171875 \t 05:53:18.229834\n",
      "Step  129 \t training loss: 1.48046875 \t 05:53:29.459822\n",
      "Step  130 \t training loss: 1.2822265625 \t 05:53:40.678012\n",
      "Step  131 \t training loss: 1.5185546875 \t 05:53:51.875922\n",
      "Step  132 \t training loss: 1.4267578125 \t 05:54:03.102965\n",
      "Step  133 \t training loss: 1.3837890625 \t 05:54:14.318240\n",
      "Step  134 \t training loss: 1.48828125 \t 05:54:25.558288\n",
      "Step  135 \t training loss: 1.46484375 \t 05:54:37.174450\n",
      "Step  136 \t training loss: 1.453125 \t 05:54:48.843404\n",
      "Step  137 \t training loss: 1.4150390625 \t 05:55:00.082039\n",
      "Step  138 \t training loss: 1.2509765625 \t 05:55:11.339269\n",
      "Step  139 \t training loss: 1.244140625 \t 05:55:22.383246\n",
      "Step  140 \t training loss: 1.4794921875 \t 05:55:33.062501\n",
      "validation loss: 1.94140625\n",
      "Step  141 \t training loss: 1.5361328125 \t 05:55:44.429804\n",
      "Step  142 \t training loss: 1.478515625 \t 05:55:55.514657\n",
      "Step  143 \t training loss: 1.431640625 \t 05:56:06.841903\n",
      "Step  144 \t training loss: 1.2978515625 \t 05:56:18.205724\n",
      "Step  145 \t training loss: 1.349609375 \t 05:56:29.045306\n",
      "Step  146 \t training loss: 1.3740234375 \t 05:56:39.864904\n",
      "Step  147 \t training loss: 1.2880859375 \t 05:56:51.099667\n",
      "Step  148 \t training loss: 1.4306640625 \t 05:57:02.307555\n",
      "Step  149 \t training loss: 1.5439453125 \t 05:57:13.538324\n",
      "Step  150 \t training loss: 1.4033203125 \t 05:57:24.752990\n",
      "Step  151 \t training loss: 1.4501953125 \t 05:57:36.323783\n",
      "Step  152 \t training loss: 1.3076171875 \t 05:57:47.918827\n",
      "Step  153 \t training loss: 1.341796875 \t 05:57:59.152871\n",
      "Step  154 \t training loss: 1.44921875 \t 05:58:10.345000\n",
      "Step  155 \t training loss: 1.357421875 \t 05:58:21.596140\n",
      "Step  156 \t training loss: 1.3212890625 \t 05:58:32.816759\n",
      "Step  157 \t training loss: 1.3115234375 \t 05:58:44.036003\n",
      "Step  158 \t training loss: 1.349609375 \t 05:58:55.284777\n",
      "Step  159 \t training loss: 1.44921875 \t 05:59:06.854893\n",
      "Step  160 \t training loss: 1.3154296875 \t 05:59:18.452034\n",
      "validation loss: 1.9453125\n",
      "Step  161 \t training loss: 1.3740234375 \t 05:59:30.459417\n",
      "Step  162 \t training loss: 1.3271484375 \t 05:59:41.693876\n",
      "Step  163 \t training loss: 1.2919921875 \t 05:59:52.923313\n",
      "Step  164 \t training loss: 1.365234375 \t 06:00:04.134602\n",
      "Step  165 \t training loss: 1.236328125 \t 06:00:15.367844\n",
      "Step  166 \t training loss: 1.3525390625 \t 06:00:26.582488\n",
      "Step  167 \t training loss: 1.3427734375 \t 06:00:38.134549\n",
      "Step  168 \t training loss: 1.2509765625 \t 06:00:49.745688\n",
      "Step  169 \t training loss: 1.2412109375 \t 06:01:00.968326\n",
      "Step  170 \t training loss: 1.283203125 \t 06:01:12.196937\n",
      "Step  171 \t training loss: 1.2626953125 \t 06:01:23.436315\n",
      "Step  172 \t training loss: 1.3828125 \t 06:01:34.650879\n",
      "Step  173 \t training loss: 1.3662109375 \t 06:01:45.856818\n",
      "Step  174 \t training loss: 1.3505859375 \t 06:01:57.126608\n",
      "Step  175 \t training loss: 1.3388671875 \t 06:02:08.691859\n",
      "Step  176 \t training loss: 1.3662109375 \t 06:02:20.265813\n",
      "Step  177 \t training loss: 1.3486328125 \t 06:02:31.484843\n",
      "Step  178 \t training loss: 1.1015625 \t 06:02:42.720179\n",
      "Step  179 \t training loss: 1.2626953125 \t 06:02:54.034526\n",
      "Step  180 \t training loss: 1.2841796875 \t 06:03:04.936125\n",
      "validation loss: 1.990234375\n",
      "**************************************************************************************************** \n",
      "Question:  !What is 489.788 - 0.2?\"                                                                                                                                                                                                                                        \n",
      "Actual Answer:  !489.588\"                                                                                                                                                                                                                                                       \n",
      "Decoded Prediction:  \"g.8\"\"1\"\"\"1\"1\"@.0\"f.1\"\"1\".2\"\"\"\".\n",
      "Step  181 \t training loss: 1.3818359375 \t 06:03:18.958370\n",
      "Step  182 \t training loss: 1.3603515625 \t 06:03:29.999956\n",
      "Step  183 \t training loss: 1.3232421875 \t 06:03:41.429307\n",
      "Step  184 \t training loss: 1.2353515625 \t 06:03:52.607947\n",
      "Step  185 \t training loss: 1.24609375 \t 06:04:03.648745\n",
      "Step  186 \t training loss: 1.314453125 \t 06:04:14.735185\n",
      "Step  187 \t training loss: 1.2041015625 \t 06:04:25.597000\n",
      "Step  188 \t training loss: 1.189453125 \t 06:04:36.392040\n",
      "Step  189 \t training loss: 1.2412109375 \t 06:04:47.293908\n",
      "Step  190 \t training loss: 1.1611328125 \t 06:04:58.394920\n",
      "Step  191 \t training loss: 1.2861328125 \t 06:05:09.887844\n",
      "Step  192 \t training loss: 1.158203125 \t 06:05:21.300037\n",
      "Step  193 \t training loss: 1.177734375 \t 06:05:32.281232\n",
      "Step  194 \t training loss: 1.1552734375 \t 06:05:43.451664\n",
      "Step  195 \t training loss: 1.2080078125 \t 06:05:54.349808\n",
      "Step  196 \t training loss: 1.22265625 \t 06:06:05.177512\n",
      "Step  197 \t training loss: 1.197265625 \t 06:06:16.301795\n",
      "Step  198 \t training loss: 1.1923828125 \t 06:06:27.522560\n",
      "Step  199 \t training loss: 1.294921875 \t 06:06:39.084422\n",
      "Step  200 \t training loss: 1.1728515625 \t 06:06:50.674021\n",
      "validation loss: 1.9541015625\n",
      "Step  201 \t training loss: 1.1201171875 \t 06:07:02.632061\n",
      "Step  202 \t training loss: 1.171875 \t 06:07:13.841859\n",
      "Step  203 \t training loss: 1.2197265625 \t 06:07:25.077565\n",
      "Step  204 \t training loss: 1.177734375 \t 06:07:36.285724\n",
      "Step  205 \t training loss: 1.0966796875 \t 06:07:47.496344\n",
      "Step  206 \t training loss: 1.1728515625 \t 06:07:58.697391\n",
      "Step  207 \t training loss: 1.130859375 \t 06:08:10.266200\n",
      "Step  208 \t training loss: 1.197265625 \t 06:08:21.843706\n",
      "Step  209 \t training loss: 1.1591796875 \t 06:08:33.211919\n",
      "Step  210 \t training loss: 1.1064453125 \t 06:08:44.549599\n",
      "Step  211 \t training loss: 1.1640625 \t 06:08:55.798955\n",
      "Step  212 \t training loss: 1.23828125 \t 06:09:07.032470\n",
      "Step  213 \t training loss: 1.0859375 \t 06:09:18.253324\n",
      "Step  214 \t training loss: 1.1015625 \t 06:09:29.473774\n",
      "Step  215 \t training loss: 1.1279296875 \t 06:09:41.044768\n",
      "Step  216 \t training loss: 1.0947265625 \t 06:09:52.654726\n",
      "Step  217 \t training loss: 1.115234375 \t 06:10:03.869961\n",
      "Step  218 \t training loss: 0.98876953125 \t 06:10:15.094177\n",
      "Step  219 \t training loss: 1.02734375 \t 06:10:26.311370\n",
      "Step  220 \t training loss: 1.26171875 \t 06:10:37.524031\n",
      "validation loss: 2.041015625\n",
      "Epoch    12: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Step  221 \t training loss: 1.119140625 \t 06:10:49.468739\n",
      "Step  222 \t training loss: 1.0263671875 \t 06:11:00.681530\n",
      "Step  223 \t training loss: 1.0654296875 \t 06:11:12.258894\n",
      "Step  224 \t training loss: 0.93017578125 \t 06:11:23.836259\n",
      "Step  225 \t training loss: 0.9501953125 \t 06:11:35.067015\n",
      "Step  226 \t training loss: 1.193359375 \t 06:11:46.282057\n",
      "Step  227 \t training loss: 0.970703125 \t 06:11:57.498189\n",
      "Step  228 \t training loss: 1.0517578125 \t 06:12:08.713295\n",
      "Step  229 \t training loss: 1.0888671875 \t 06:12:19.928980\n",
      "Step  230 \t training loss: 0.96923828125 \t 06:12:31.151272\n",
      "Step  231 \t training loss: 1.0615234375 \t 06:12:42.712209\n",
      "Step  232 \t training loss: 1.052734375 \t 06:12:54.317410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  233 \t training loss: 1.0654296875 \t 06:13:05.543891\n",
      "Step  234 \t training loss: 0.9765625 \t 06:13:16.756509\n",
      "Step  235 \t training loss: 0.99365234375 \t 06:13:27.983585\n",
      "Step  236 \t training loss: 0.9521484375 \t 06:13:39.214792\n",
      "Step  237 \t training loss: 0.9951171875 \t 06:13:50.422750\n",
      "Step  238 \t training loss: 1.0029296875 \t 06:14:01.637970\n",
      "Step  239 \t training loss: 0.95068359375 \t 06:14:13.215522\n",
      "Step  240 \t training loss: 1.001953125 \t 06:14:24.800805\n",
      "validation loss: 2.02734375\n",
      "**************************************************************************************************** \n",
      "Question:  !Evaluate (1 - 2) + -1 - (-2 - 1).\"                                                                                                                                                                                                                             \n",
      "Actual Answer:  !1\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  \"\"\"\"\"\"\"\"\"Q1\"\"\"\"\"\"\"\"\"\"\"\"1\"\"\"\"\"\"\"\"\n",
      "Step  241 \t training loss: 1.0107421875 \t 06:14:38.898319\n",
      "Step  242 \t training loss: 1.0498046875 \t 06:14:50.114003\n",
      "Step  243 \t training loss: 0.95458984375 \t 06:15:01.342966\n",
      "Step  244 \t training loss: 0.99365234375 \t 06:15:12.582148\n",
      "Step  245 \t training loss: 1.005859375 \t 06:15:23.806210\n",
      "Step  246 \t training loss: 0.9248046875 \t 06:15:35.048866\n",
      "Step  247 \t training loss: 0.96484375 \t 06:15:46.628374\n",
      "Step  248 \t training loss: 0.8740234375 \t 06:15:58.228990\n",
      "Step  249 \t training loss: 0.962890625 \t 06:16:09.463970\n",
      "Step  250 \t training loss: 0.96484375 \t 06:16:20.678750\n",
      "Step  251 \t training loss: 0.9052734375 \t 06:16:31.890317\n",
      "Step  252 \t training loss: 1.0244140625 \t 06:16:43.126499\n",
      "Step  253 \t training loss: 1.0361328125 \t 06:16:54.348857\n",
      "Step  254 \t training loss: 0.9931640625 \t 06:17:05.577214\n",
      "Step  255 \t training loss: 0.962890625 \t 06:17:17.188369\n",
      "Step  256 \t training loss: 0.91015625 \t 06:17:28.819437\n",
      "Step  257 \t training loss: 1.0283203125 \t 06:17:40.048692\n",
      "Step  258 \t training loss: 0.9990234375 \t 06:17:51.281619\n",
      "Step  259 \t training loss: 0.86962890625 \t 06:18:02.493449\n",
      "Step  260 \t training loss: 0.8671875 \t 06:18:13.709604\n",
      "validation loss: 2.03125\n",
      "Step  261 \t training loss: 0.95263671875 \t 06:18:25.651443\n",
      "Step  262 \t training loss: 0.99169921875 \t 06:18:36.891241\n",
      "Step  263 \t training loss: 0.9765625 \t 06:18:48.505638\n",
      "Step  264 \t training loss: 0.92431640625 \t 06:19:00.141616\n",
      "Step  265 \t training loss: 1.0146484375 \t 06:19:11.387413\n",
      "Step  266 \t training loss: 0.87158203125 \t 06:19:22.655449\n",
      "Step  267 \t training loss: 0.865234375 \t 06:19:33.871686\n",
      "Step  268 \t training loss: 0.8896484375 \t 06:19:45.103983\n",
      "Step  269 \t training loss: 0.95263671875 \t 06:19:56.371565\n",
      "Step  270 \t training loss: 1.05078125 \t 06:20:07.622514\n",
      "Step  271 \t training loss: 0.95703125 \t 06:20:19.264145\n",
      "Step  272 \t training loss: 0.8984375 \t 06:20:30.883448\n",
      "Step  273 \t training loss: 0.93505859375 \t 06:20:42.103079\n",
      "Step  274 \t training loss: 0.90087890625 \t 06:20:53.330839\n",
      "Step  275 \t training loss: 0.9423828125 \t 06:21:04.553734\n",
      "Step  276 \t training loss: 0.958984375 \t 06:21:15.778851\n",
      "Step  277 \t training loss: 0.95263671875 \t 06:21:27.008108\n",
      "Step  278 \t training loss: 0.9501953125 \t 06:21:38.230539\n",
      "Step  279 \t training loss: 0.87451171875 \t 06:21:49.809057\n",
      "Step  280 \t training loss: 0.89306640625 \t 06:22:01.417253\n",
      "validation loss: 2.060546875\n",
      "Step  281 \t training loss: 0.99853515625 \t 06:22:13.388280\n",
      "Step  282 \t training loss: 1.001953125 \t 06:22:24.615733\n",
      "Step  283 \t training loss: 0.78564453125 \t 06:22:35.840816\n",
      "Step  284 \t training loss: 0.93212890625 \t 06:22:47.080149\n",
      "Step  285 \t training loss: 0.87841796875 \t 06:22:58.278324\n",
      "Step  286 \t training loss: 0.9677734375 \t 06:23:09.502863\n",
      "Step  287 \t training loss: 0.87890625 \t 06:23:21.068981\n",
      "Step  288 \t training loss: 1.0029296875 \t 06:23:32.658164\n",
      "Step  289 \t training loss: 0.90087890625 \t 06:23:43.865733\n",
      "Step  290 \t training loss: 0.97802734375 \t 06:23:55.082500\n",
      "Step  291 \t training loss: 0.90966796875 \t 06:24:06.291128\n",
      "Step  292 \t training loss: 0.86962890625 \t 06:24:17.512745\n",
      "Step  293 \t training loss: 0.90625 \t 06:24:28.728250\n",
      "Step  294 \t training loss: 0.849609375 \t 06:24:39.933204\n",
      "Step  295 \t training loss: 0.88525390625 \t 06:24:51.516265\n",
      "Step  296 \t training loss: 0.95166015625 \t 06:25:03.114020\n",
      "Step  297 \t training loss: 0.85693359375 \t 06:25:14.333339\n",
      "Step  298 \t training loss: 0.98779296875 \t 06:25:25.555431\n",
      "Step  299 \t training loss: 0.9013671875 \t 06:25:36.770604\n",
      "Step  300 \t training loss: 0.88330078125 \t 06:25:48.001430\n",
      "validation loss: 2.1015625\n",
      "**************************************************************************************************** \n",
      "Question:  !1.93 + 2\"                                                                                                                                                                                                                                                      \n",
      "Actual Answer:  !3.93\"                                                                                                                                                                                                                                                          \n",
      "Decoded Prediction:  \"\"\"\"\".8\"\"\"\".\"\"\"\"\"\"\"\"f.8.8\"\"\"\"\"\"\"\n",
      "Step  301 \t training loss: 0.935546875 \t 06:26:02.165834\n",
      "Step  302 \t training loss: 0.78125 \t 06:26:13.448443\n",
      "Step  303 \t training loss: 0.82080078125 \t 06:26:25.102528\n",
      "Step  304 \t training loss: 0.85107421875 \t 06:26:36.726510\n",
      "Step  305 \t training loss: 0.8115234375 \t 06:26:47.951341\n",
      "Step  306 \t training loss: 0.89013671875 \t 06:26:59.195560\n",
      "Step  307 \t training loss: 0.92529296875 \t 06:27:10.420854\n",
      "Step  308 \t training loss: 0.88330078125 \t 06:27:21.659649\n",
      "Step  309 \t training loss: 0.845703125 \t 06:27:32.887472\n",
      "Step  310 \t training loss: 0.94970703125 \t 06:27:44.118207\n",
      "Step  311 \t training loss: 0.83544921875 \t 06:27:55.695725\n",
      "Step  312 \t training loss: 0.7705078125 \t 06:28:07.339806\n",
      "Step  313 \t training loss: 0.88427734375 \t 06:28:18.569297\n",
      "Step  314 \t training loss: 0.859375 \t 06:28:29.815532\n",
      "Step  315 \t training loss: 0.8623046875 \t 06:28:41.081696\n",
      "Step  316 \t training loss: 0.9560546875 \t 06:28:52.324608\n",
      "Step  317 \t training loss: 0.7998046875 \t 06:29:03.565089\n",
      "Step  318 \t training loss: 0.84521484375 \t 06:29:14.793650\n",
      "Step  319 \t training loss: 0.92236328125 \t 06:29:26.368175\n",
      "Step  320 \t training loss: 0.77294921875 \t 06:29:37.960069\n",
      "validation loss: 2.12109375\n",
      "Epoch    17: reducing learning rate of group 0 to 9.0000e-06.\n",
      "Step  321 \t training loss: 0.8642578125 \t 06:29:49.893463\n",
      "Step  322 \t training loss: 0.89453125 \t 06:30:01.144141\n",
      "Step  323 \t training loss: 0.822265625 \t 06:30:12.383038\n",
      "Step  324 \t training loss: 0.83447265625 \t 06:30:23.642442\n",
      "Step  325 \t training loss: 0.9169921875 \t 06:30:34.874894\n",
      "Step  326 \t training loss: 0.837890625 \t 06:30:46.109484\n",
      "Step  327 \t training loss: 0.7587890625 \t 06:30:57.685530\n",
      "Step  328 \t training loss: 0.86083984375 \t 06:31:09.290375\n",
      "Step  329 \t training loss: 0.86767578125 \t 06:31:20.560932\n",
      "Step  330 \t training loss: 0.7392578125 \t 06:31:31.803721\n",
      "Step  331 \t training loss: 0.82275390625 \t 06:31:43.056812\n",
      "Step  332 \t training loss: 0.81884765625 \t 06:31:54.286343\n",
      "Step  333 \t training loss: 0.86328125 \t 06:32:05.534128\n",
      "Step  334 \t training loss: 0.8916015625 \t 06:32:16.774896\n",
      "Step  335 \t training loss: 0.74658203125 \t 06:32:28.375253\n",
      "Step  336 \t training loss: 0.8701171875 \t 06:32:40.004246\n",
      "Step  337 \t training loss: 0.7890625 \t 06:32:51.252450\n",
      "Step  338 \t training loss: 0.77880859375 \t 06:33:02.500222\n",
      "Step  339 \t training loss: 0.86669921875 \t 06:33:13.741216\n",
      "Step  340 \t training loss: 0.77880859375 \t 06:33:24.977687\n",
      "validation loss: 2.126953125\n",
      "Step  341 \t training loss: 0.80029296875 \t 06:33:36.954918\n",
      "Step  342 \t training loss: 0.8876953125 \t 06:33:48.204856\n",
      "Step  343 \t training loss: 0.8203125 \t 06:33:59.795974\n",
      "Step  344 \t training loss: 0.8564453125 \t 06:34:11.448538\n",
      "Step  345 \t training loss: 0.74951171875 \t 06:34:22.677416\n",
      "Step  346 \t training loss: 0.91943359375 \t 06:34:33.919933\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "i = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "# for batch_idx, batch in enumerate(tqdm(train_loader, mininterval=2, leave=False)):\n",
    "    batch_qs, batch_qs_mask, batch_as, batch_as_mask = map(lambda x: x.to(device), next(train_loader))\n",
    "    # exclude the 0th element as it is BOS\n",
    "    gold_as = batch_as[:, 1:]\n",
    "    \n",
    "    if (i % GENERATE_EVERY) - 1 == 0:\n",
    "        enc_dec.eval()\n",
    "        gen_qs, gen_qs_mask, gen_as, gen_as_mask = next(gen_loader)\n",
    "    #         inp = random.choice(val_ds)[:-1]\n",
    "        prime = np_decode_string(gen_qs.numpy())\n",
    "        print('*' * 100, \"\\nQuestion: \", prime)\n",
    "        print(\"Actual Answer: \", np_decode_string(gen_as.numpy()))\n",
    "    #     print(\"Raw Answer: \", gen_as.numpy())\n",
    "        gen_qs = gen_qs.to(device)\n",
    "        gen_as = gen_as.to(device)\n",
    "        gen_qs_mask = gen_qs_mask.to(device)\n",
    "        sample = enc_dec.generate(gen_qs, gen_as, GENERATE_LENGTH, enc_input_mask = gen_qs_mask)\n",
    "        sample = sample.cpu().numpy()\n",
    "        output_str = np_decode_string(sample)\n",
    "    #     print(\"Raw Prediction: \", sample)\n",
    "        print(\"Decoded Prediction: \", output_str)\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-train_loss.txt\", train_loss_list)\n",
    "        np.savetxt(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\", val_loss_list)\n",
    "        \n",
    "#         with open(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-train_loss.txt\", \"w\") as fp:\n",
    "#             pickle.dumps(train_loss_list, fp)\n",
    "#         with open(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\", \"w\") as fp:\n",
    "#             pickle.dumps(val_loss_list, fp)\n",
    "#         print(\"Logs saved to \", \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\")\n",
    "            \n",
    "            \n",
    "\n",
    "    enc_dec.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        train_loss = enc_dec(batch_qs, batch_as, return_loss = True, enc_input_mask = batch_qs_mask)\n",
    "        with amp.scale_loss(train_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    \n",
    "#     if batch_idx % GRADIENT_ACCUMULATE_EVERY == 0:\n",
    "    print(\"Step \", i, \"\\t\", f'training loss: {train_loss.item()}', \"\\t\", datetime.now().time() )\n",
    "    train_loss_list.append((i, train_loss.item()))\n",
    "    torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 0.1)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        val_batch_qs, val_batch_qs_mask, val_batch_as, val_batch_as_mask = map(lambda x: x.to(device), next(val_loader))\n",
    "        enc_dec.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = enc_dec(val_batch_qs, val_batch_as, return_loss = True, enc_input_mask = val_batch_qs_mask)\n",
    "            print(f'validation loss: {val_loss.item()}')\n",
    "            val_loss_list.append((i, val_loss.item()))\n",
    "            scheduler.step(val_loss)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([train_loss_list[1]])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
