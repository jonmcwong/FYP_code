{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version 1.5.0\n"
     ]
    }
   ],
   "source": [
    "# code based off of \n",
    "# https://github.com/mandubian/pytorch_math_dataset and\n",
    "# https://github.com/lucidrains/reformer-pytorch\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "import tqdm as tqdm\n",
    "import random\n",
    "from datetime import datetime\n",
    "from apex import amp\n",
    "import pickle\n",
    "\n",
    "\n",
    "import mandubian.math_dataset\n",
    "from mandubian.math_dataset import MathDatasetManager\n",
    "from mandubian.transformer import Constants\n",
    "\n",
    "# from transformer.Models import Transformer\n",
    "from mandubian.math_dataset import (\n",
    "    random_split_dataset,\n",
    "    question_answer_to_mask_batch_collate_fn\n",
    ")\n",
    "from mandubian.math_dataset import np_encode_string, np_decode_string\n",
    "import mandubian.model_process\n",
    "import mandubian.utils\n",
    "from mandubian.tensorboard_utils import Tensorboard\n",
    "from mandubian.tensorboard_utils import tensorboard_event_accumulator\n",
    "\n",
    "import mandubian.checkpoints\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Torch Version\", torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 detected CUDA devices\n",
      "Using CUDA device:  0\n",
      "GeForce RTX 2080\n",
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 1\n",
    "print(torch.cuda.device_count(), \"detected CUDA devices\")\n",
    "cuda_device = torch.cuda.current_device()\n",
    "print(\"Using CUDA device: \", cuda_device)\n",
    "print(torch.cuda.get_device_name(cuda_device))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucidrains_reformer.reformer_pytorch import ReformerLM, Autopadder, Recorder\n",
    "from lucidrains_reformer.reformer_pytorch import ReformerEncDec\n",
    "from lucidrains_reformer.reformer_pytorch.generative_tools import TrainingWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Math Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized MultiFilesMathDataset with categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus'] and types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "mdsmgr structure ['__add__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_datasets_from_category', 'build_dataset_from_categories', 'build_dataset_from_category', 'build_dataset_from_module', 'build_dataset_from_modules', 'dfs', 'dirs', 'get_categories', 'get_modules_for_category', 'get_types', 'root_dir']\n"
     ]
    }
   ],
   "source": [
    "mdsmgr = MathDatasetManager(\n",
    "  \"/home/jonathan/Repos/final_year_at_ic/awesome_project/mathematics_dataset-v1.0/\"\n",
    ")\n",
    "# Examine dataset structure\n",
    "print(\"mdsmgr structure\", dir(mdsmgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MathDatasetManager._build_datasets_from_category of <mandubian.math_dataset.MathDatasetManager object at 0x7efb77bcdc88>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(MathDatasetManager.__dir__\n",
    "mdsmgr._build_datasets_from_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check availables types, problem categories and problem subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "types ['train-easy', 'train-medium', 'train-hard', 'interpolate', 'extrapolate']\n",
      "categories ['algebra', 'numbers', 'polynomials', 'comparison', 'arithmetic', 'measurement', 'probability', 'calculus']\n",
      "modules of arithmetic dict_keys(['div', 'nearest_integer_root', 'mul_div_multiple', 'mul', 'add_or_sub', 'add_sub_multiple', 'mixed', 'add_or_sub_in_base', 'simplify_surd', 'add_or_sub_big', 'add_sub_multiple_longer', 'mixed_longer', 'div_big', 'mul_div_multiple_longer', 'mul_big'])\n"
     ]
    }
   ],
   "source": [
    "print(\"types\", list(mdsmgr.get_types()))\n",
    "print(\"categories\", list(mdsmgr.get_categories()))\n",
    "print(\"modules of arithmetic\", mdsmgr.get_modules_for_category('arithmetic'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to manipulate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding category arithmetic/../interpolate\n",
      "added module arithmetic/div/interpolate\n",
      "added module arithmetic/nearest_integer_root/interpolate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonathan/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added module arithmetic/mul_div_multiple/interpolate\n",
      "added module arithmetic/mul/interpolate\n",
      "added module arithmetic/add_or_sub/interpolate\n",
      "added module arithmetic/add_sub_multiple/interpolate\n",
      "added module arithmetic/mixed/interpolate\n",
      "added module arithmetic/add_or_sub_in_base/interpolate\n",
      "added module arithmetic/simplify_surd/interpolate\n"
     ]
    }
   ],
   "source": [
    "# # Build Dataset from a single module in a category\n",
    "# ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from a single module in a category with limited number of elements\n",
    "# ds = mdsmgr.build_dataset_from_module('arithmetic', 'add_or_sub', 'train-easy', max_elements=1000)\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from several modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in a category\n",
    "# ds = mdsmgr.build_dataset_from_category('arithmetic', 'train-easy')\n",
    "ds = mdsmgr.build_dataset_from_category('arithmetic', 'interpolate')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # Build Dataset from all modules in several categories\n",
    "# ds = mdsmgr.build_dataset_from_categories(['arithmetic', 'polynomials'], 'train-easy')\n",
    "# print(\"size\", len(ds))\n",
    "\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"baseline_overfit_experiment\"\n",
    "now = datetime.now()\n",
    "unique_id = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "base_dir = \"/home/jonathan/Repos/final_year_at_ic/awesome_project/code/tests/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mandubian.math_dataset import (\n",
    "    VOCAB_SZ, MAX_QUESTION_SZ, MAX_ANSWER_SZ\n",
    ")\n",
    "\n",
    "NUM_CPU_THREADS = 12\n",
    "BATCH_SIZE = 128\n",
    "NUM_BATCHES = int(1e5)\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "VALIDATE_EVERY  = 20\n",
    "GENERATE_EVERY  = 60\n",
    "GENERATE_LENGTH = 32\n",
    "\n",
    "# hyperparameters need updates\n",
    "\n",
    "Q_SEQ_LEN = 256\n",
    "A_SEQ_LEN = 30 # unused due to requirements of axial_positon_shape\n",
    "NUM_TOKENS = VOCAB_SZ + 1\n",
    "D_MODEL = 512\n",
    "EMB_DIM = D_MODEL\n",
    "NUM_HEADS = 8\n",
    "QKV_DIM = D_MODEL / NUM_HEADS\n",
    "NUM_LAYERS = 6\n",
    "D_FF = 2048\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "# training_data = mdsmgr.build_dataset_from_category('arithmetic','train-easy') # for now\n",
    "training_data = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'train-easy')\n",
    "\n",
    "# testing data\n",
    "# testing_data_interpolate = mdsmgr.build_dataset_from_category('arithmetic','interpolate')\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_category('arithmetic','extrapolate')\n",
    "\n",
    "testing_data_interpolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'interpolate')\n",
    "# testing_data_extrapolate = mdsmgr.build_dataset_from_modules('arithmetic', ['add_or_sub', 'add_sub_multiple'], 'extrapolate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lucidrains_reformer.examples.enwik8_simple.train\n",
    "# helpers\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "def decode_token(token):\n",
    "    return str(chr(max(32, token)))\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return ''.join(list(map(decode_token, tokens)))\n",
    "\n",
    "def get_non_pad_mask(seq):\n",
    "    # returns true when token is not PAD and false otherwise\n",
    "    assert seq.dim() == 2\n",
    "    return seq.ne(Constants.PAD).type(torch.float).unsqueeze(-1)\n",
    "\n",
    "# get data splits\n",
    "train_ds, val_ds = mandubian.math_dataset.random_split_dataset(training_data,split_rate=0.9)\n",
    "\n",
    "# get pytorch dataloaders\n",
    "# Questions are padded in question_answer_to_position_batch_collate_fn\n",
    "train_loader = data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "train_loader = cycle(train_loader)\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "val_loader = cycle(val_loader)\n",
    "\n",
    "# for viewing output sequences\n",
    "gen_loader = data.DataLoader(\n",
    "    val_ds, batch_size=1, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "gen_loader = cycle(gen_loader)\n",
    "\n",
    "interpolate_loader = data.DataLoader(\n",
    "    testing_data_interpolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "    collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "interpolate_loader = cycle(interpolate_loader)\n",
    "\n",
    "# extrapolate_loader = data.DataLoader(\n",
    "#     testing_data_extrapolate, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_CPU_THREADS,\n",
    "#     collate_fn=question_answer_to_mask_batch_collate_fn)\n",
    "# extrapolate_loader = cycle(extrapolate_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerEncDec(\n",
       "  (enc): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x4x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x64x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): TrainingWrapper(\n",
       "    (net): Autopadder(\n",
       "      (net): ReformerLM(\n",
       "        (token_emb): Embedding(96, 512, padding_idx=0)\n",
       "        (to_model_dim): Identity()\n",
       "        (pos_emb): AxialPositionalEncoding(\n",
       "          (weights): ParameterList(\n",
       "              (0): Parameter containing: [torch.cuda.FloatTensor of size 1x2x1x256 (GPU 0)]\n",
       "              (1): Parameter containing: [torch.cuda.FloatTensor of size 1x1x128x256 (GPU 0)]\n",
       "          )\n",
       "        )\n",
       "        (reformer): Reformer(\n",
       "          (layers): ReversibleSequence(\n",
       "            (blocks): ModuleList(\n",
       "              (0): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): ReversibleBlock(\n",
       "                (f): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): LSHSelfAttention(\n",
       "                      (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                      (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                      (lsh_attn): LSHAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (full_attn): FullQKAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (local_attn): LocalAttention(\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): Deterministic(\n",
       "                  (net): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Chunk(\n",
       "                      (fn): FeedForward(\n",
       "                        (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                        (act): GELU()\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                        (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                      )\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (irrev_blocks): ModuleList(\n",
       "              (0): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): IrreversibleBlock(\n",
       "                (f): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): LSHSelfAttention(\n",
       "                    (toqk): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (tov): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lsh_attn): LSHAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (full_attn): FullQKAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (local_attn): LocalAttention(\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (g): PreNorm(\n",
       "                  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Chunk(\n",
       "                    (fn): FeedForward(\n",
       "                      (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                      (act): GELU()\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (out): Sequential(\n",
       "          (0): Identity()\n",
       "          (1): Linear(in_features=512, out_features=96, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "enc_dec = ReformerEncDec(\n",
    "    dim = D_MODEL,\n",
    "    enc_num_tokens = NUM_TOKENS,\n",
    "    enc_depth = NUM_LAYERS,\n",
    "    enc_max_seq_len = Q_SEQ_LEN,\n",
    "    dec_num_tokens = NUM_TOKENS,\n",
    "    dec_depth = NUM_LAYERS,\n",
    "    dec_max_seq_len = Q_SEQ_LEN,\n",
    "    # heads = 8 by default\n",
    "    axial_position_shape = (64, 16),  # the shape must multiply up to the max_seq_len (128 x 64 = 8192)\n",
    "    axial_position_dims = (256,256),   # the dims must sum up to the model dimensions (512 + 512 = 1024)\n",
    "    pad_value = Constants.PAD,\n",
    "    ignore_index = Constants.PAD # see if this works. pad_value and ignore_index are probably different\n",
    ").cuda()\n",
    "\n",
    "# enc_dec = Recorder(enc_dec)\n",
    "enc_dec.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer learning rate scheduler, mixed precision setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(enc_dec.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.995), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=4, verbose=True)\n",
    "\n",
    "# mixed precision\n",
    "enc_dec, optimizer = amp.initialize(enc_dec, optimizer, opt_level='O2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 \t training loss: 4.96484375 \t 22:24:19.759705\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "validation loss: 5.0390625\n",
      "**************************************************************************************************** \n",
      "Question:  !Evaluate 13 + (4 + -16 - 9).\"                                                                                                                                                                                                                                  \n",
      "Actual Answer:  !-8\"                                                                                                                                                                                                                                                            \n",
      "Decoded Prediction:  i765 isslZl6P %heVi%lZilsfsHfil6\n",
      "Logs saved to  logs/baseline_overfit_experiment_05-17-2020_22-24-05-val_loss.txt\n",
      "Step  1 \t training loss: 5.09375 \t 22:24:34.081874\n",
      "Step  2 \t training loss: 4.15234375 \t 22:24:45.420917\n",
      "Step  3 \t training loss: 3.451171875 \t 22:24:56.639583\n",
      "Step  4 \t training loss: 2.908203125 \t 22:25:07.880009\n",
      "Step  5 \t training loss: 2.67578125 \t 22:25:19.116864\n",
      "Step  6 \t training loss: 2.47265625 \t 22:25:30.367012\n",
      "Step  7 \t training loss: 2.447265625 \t 22:25:41.589698\n",
      "Step  8 \t training loss: 2.34375 \t 22:25:52.822346\n",
      "Step  9 \t training loss: 2.22265625 \t 22:26:04.066941\n",
      "Step  10 \t training loss: 2.283203125 \t 22:26:15.307783\n",
      "Step  11 \t training loss: 2.298828125 \t 22:26:26.555539\n",
      "Step  12 \t training loss: 2.197265625 \t 22:26:37.855171\n",
      "Step  13 \t training loss: 2.271484375 \t 22:26:49.123944\n",
      "Step  14 \t training loss: 2.431640625 \t 22:27:00.394143\n",
      "Step  15 \t training loss: 2.037109375 \t 22:27:11.679351\n",
      "Step  16 \t training loss: 2.083984375 \t 22:27:22.982487\n",
      "Step  17 \t training loss: 2.115234375 \t 22:27:34.205877\n",
      "Step  18 \t training loss: 2.12890625 \t 22:27:45.562621\n",
      "Step  19 \t training loss: 1.9873046875 \t 22:27:56.916412\n",
      "Step  20 \t training loss: 2.033203125 \t 22:28:08.124794\n",
      "validation loss: 2.05078125\n",
      "Step  21 \t training loss: 2.185546875 \t 22:28:19.804032\n",
      "Step  22 \t training loss: 2.1015625 \t 22:28:31.018714\n",
      "Step  23 \t training loss: 1.982421875 \t 22:28:42.234299\n",
      "Step  24 \t training loss: 1.978515625 \t 22:28:53.653474\n",
      "Step  25 \t training loss: 1.865234375 \t 22:29:04.984582\n",
      "Step  26 \t training loss: 2.1171875 \t 22:29:16.428505\n",
      "Step  27 \t training loss: 1.927734375 \t 22:29:27.967602\n",
      "Step  28 \t training loss: 1.9931640625 \t 22:29:39.479133\n",
      "Step  29 \t training loss: 1.8466796875 \t 22:29:50.986982\n",
      "Step  30 \t training loss: 1.8935546875 \t 22:30:02.501716\n",
      "Step  31 \t training loss: 2.04296875 \t 22:30:14.028514\n",
      "Step  32 \t training loss: 1.8984375 \t 22:30:25.340586\n",
      "Step  33 \t training loss: 2.04296875 \t 22:30:36.583411\n",
      "Step  34 \t training loss: 1.9873046875 \t 22:30:47.862479\n",
      "Step  35 \t training loss: 1.9345703125 \t 22:30:59.235221\n",
      "Step  36 \t training loss: 1.96484375 \t 22:31:10.479522\n",
      "Step  37 \t training loss: 2.0078125 \t 22:31:21.817256\n",
      "Step  38 \t training loss: 1.998046875 \t 22:31:33.017856\n",
      "Step  39 \t training loss: 1.9443359375 \t 22:31:44.220533\n",
      "Step  40 \t training loss: 1.875 \t 22:31:55.567915\n",
      "validation loss: 1.806640625\n",
      "Step  41 \t training loss: 2.037109375 \t 22:32:07.054304\n",
      "Step  42 \t training loss: 2.064453125 \t 22:32:18.011820\n",
      "Step  43 \t training loss: 1.9208984375 \t 22:32:28.968171\n",
      "Step  44 \t training loss: 1.890625 \t 22:32:40.024625\n",
      "Step  45 \t training loss: 1.876953125 \t 22:32:50.976083\n",
      "Step  46 \t training loss: 1.9111328125 \t 22:33:01.939421\n",
      "Step  47 \t training loss: 1.9287109375 \t 22:33:12.764473\n",
      "Step  48 \t training loss: 1.81640625 \t 22:33:23.548986\n",
      "Step  49 \t training loss: 1.93359375 \t 22:33:34.436225\n",
      "Step  50 \t training loss: 2.05859375 \t 22:33:45.700254\n",
      "Step  51 \t training loss: 1.8427734375 \t 22:33:56.972201\n",
      "Step  52 \t training loss: 1.9716796875 \t 22:34:08.213447\n",
      "Step  53 \t training loss: 1.9013671875 \t 22:34:19.481019\n",
      "Step  54 \t training loss: 2.001953125 \t 22:34:30.804981\n",
      "Step  55 \t training loss: 1.93359375 \t 22:34:42.058578\n",
      "Step  56 \t training loss: 1.9375 \t 22:34:53.342645\n",
      "Step  57 \t training loss: 1.9619140625 \t 22:35:04.566736\n",
      "Step  58 \t training loss: 1.9521484375 \t 22:35:15.784433\n",
      "Step  59 \t training loss: 1.89453125 \t 22:35:26.983003\n",
      "Step  60 \t training loss: 1.9443359375 \t 22:35:38.282588\n",
      "validation loss: 1.9150390625\n",
      "**************************************************************************************************** \n",
      "Question:  !What is 0.2 take away 0.8232?\"                                                                                                                                                                                                                                 \n",
      "Actual Answer:  !-0.6232\"                                                                                                                                                                                                                                                       \n",
      "Decoded Prediction:  65\"4\"ll6\"\"\"7\"\"\"\"7\"\"\"\"|\"4\"4\"\"3\"|7\n",
      "Logs saved to  logs/baseline_overfit_experiment_05-17-2020_22-24-05-val_loss.txt\n",
      "Step  61 \t training loss: 1.841796875 \t 22:35:52.248498\n",
      "Step  62 \t training loss: 1.822265625 \t 22:36:03.490844\n",
      "Step  63 \t training loss: 1.8125 \t 22:36:14.707428\n",
      "Step  64 \t training loss: 1.9326171875 \t 22:36:25.978329\n",
      "Step  65 \t training loss: 1.966796875 \t 22:36:37.203038\n",
      "Step  66 \t training loss: 2.03125 \t 22:36:48.438557\n",
      "Step  67 \t training loss: 1.8349609375 \t 22:36:59.666112\n",
      "Step  68 \t training loss: 1.896484375 \t 22:37:10.922784\n",
      "Step  69 \t training loss: 1.8994140625 \t 22:37:22.145617\n",
      "Step  70 \t training loss: 1.853515625 \t 22:37:33.379592\n",
      "Step  71 \t training loss: 1.8671875 \t 22:37:44.618655\n",
      "Step  72 \t training loss: 1.85546875 \t 22:37:55.873807\n",
      "Step  73 \t training loss: 2.048828125 \t 22:38:07.182884\n",
      "Step  74 \t training loss: 1.74609375 \t 22:38:18.466944\n",
      "Step  75 \t training loss: 1.8974609375 \t 22:38:29.814962\n",
      "Step  76 \t training loss: 1.9384765625 \t 22:38:41.165960\n",
      "Step  77 \t training loss: 1.8681640625 \t 22:38:52.159878\n",
      "Step  78 \t training loss: 1.9462890625 \t 22:39:03.282850\n",
      "Step  79 \t training loss: 1.845703125 \t 22:39:14.046668\n",
      "Step  80 \t training loss: 2.05859375 \t 22:39:24.806125\n",
      "validation loss: 1.830078125\n",
      "Step  81 \t training loss: 1.8583984375 \t 22:39:35.931447\n",
      "Step  82 \t training loss: 1.984375 \t 22:39:46.691651\n",
      "Step  83 \t training loss: 1.9189453125 \t 22:39:57.447853\n",
      "Step  84 \t training loss: 2.0546875 \t 22:40:08.260060\n",
      "Step  85 \t training loss: 1.83984375 \t 22:40:19.014003\n",
      "Step  86 \t training loss: 2.134765625 \t 22:40:29.764365\n",
      "Step  87 \t training loss: 2.0703125 \t 22:40:40.596690\n",
      "Step  88 \t training loss: 1.7392578125 \t 22:40:51.358185\n",
      "Step  89 \t training loss: 1.9638671875 \t 22:41:02.105998\n",
      "Step  90 \t training loss: 1.7890625 \t 22:41:12.857790\n",
      "Step  91 \t training loss: 1.7822265625 \t 22:41:23.609174\n",
      "Step  92 \t training loss: 2.0078125 \t 22:41:34.360940\n",
      "Step  93 \t training loss: 1.80859375 \t 22:41:45.111224\n",
      "Step  94 \t training loss: 1.9150390625 \t 22:41:55.907300\n",
      "Step  95 \t training loss: 1.8115234375 \t 22:42:06.754472\n",
      "Step  96 \t training loss: 1.9296875 \t 22:42:17.510090\n",
      "Step  97 \t training loss: 1.8193359375 \t 22:42:28.421641\n",
      "Step  98 \t training loss: 1.94921875 \t 22:42:39.415779\n",
      "Step  99 \t training loss: 1.8623046875 \t 22:42:50.258300\n",
      "Step  100 \t training loss: 1.7841796875 \t 22:43:01.182674\n",
      "validation loss: 1.8740234375\n",
      "Step  101 \t training loss: 1.8662109375 \t 22:43:12.501980\n",
      "Step  102 \t training loss: 1.876953125 \t 22:43:23.620539\n",
      "Step  103 \t training loss: 1.9462890625 \t 22:43:34.477525\n",
      "Step  104 \t training loss: 1.8935546875 \t 22:43:45.427112\n",
      "Step  105 \t training loss: 2.0078125 \t 22:43:56.473995\n",
      "Step  106 \t training loss: 1.8857421875 \t 22:44:07.447779\n",
      "Step  107 \t training loss: 1.947265625 \t 22:44:18.349689\n",
      "Step  108 \t training loss: 1.8505859375 \t 22:44:29.102707\n",
      "Step  109 \t training loss: 1.751953125 \t 22:44:40.108559\n",
      "Step  110 \t training loss: 1.8623046875 \t 22:44:51.204101\n",
      "Step  111 \t training loss: 1.8544921875 \t 22:45:02.340062\n",
      "Step  112 \t training loss: 1.6923828125 \t 22:45:13.223909\n",
      "Step  113 \t training loss: 1.90625 \t 22:45:24.272173\n",
      "Step  114 \t training loss: 1.896484375 \t 22:45:35.356248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  115 \t training loss: 1.8740234375 \t 22:45:46.133920\n",
      "Step  116 \t training loss: 1.9140625 \t 22:45:56.895654\n",
      "Step  117 \t training loss: 1.8662109375 \t 22:46:07.868763\n",
      "Step  118 \t training loss: 2.0 \t 22:46:18.698815\n",
      "Step  119 \t training loss: 1.7080078125 \t 22:46:29.482913\n",
      "Step  120 \t training loss: 1.759765625 \t 22:46:40.275430\n",
      "validation loss: 1.9638671875\n",
      "**************************************************************************************************** \n",
      "Question:  !6 - (-10 + 13) - 1\"                                                                                                                                                                                                                                            \n",
      "Actual Answer:  !2\"                                                                                                                                                                                                                                                             \n",
      "Decoded Prediction:  \"\"\"\"\"\"\"\"\"\"\"3\"\"\"\"\"\"\"N\"\"\"N\"\"\"\"\"\"\"\"\n",
      "Logs saved to  logs/baseline_overfit_experiment_05-17-2020_22-24-05-val_loss.txt\n",
      "Step  121 \t training loss: 2.02734375 \t 22:46:53.683657\n",
      "Step  122 \t training loss: 1.7314453125 \t 22:47:04.686112\n",
      "Step  123 \t training loss: 1.763671875 \t 22:47:15.526016\n",
      "Step  124 \t training loss: 1.8037109375 \t 22:47:26.346106\n",
      "Step  125 \t training loss: 1.8486328125 \t 22:47:37.130147\n",
      "Step  126 \t training loss: 1.8369140625 \t 22:47:47.957625\n",
      "Step  127 \t training loss: 1.74609375 \t 22:47:58.746973\n",
      "Step  128 \t training loss: 1.8466796875 \t 22:48:09.539162\n",
      "Step  129 \t training loss: 1.740234375 \t 22:48:20.395055\n",
      "Step  130 \t training loss: 1.931640625 \t 22:48:31.292195\n",
      "Step  131 \t training loss: 1.8466796875 \t 22:48:42.096937\n",
      "Step  132 \t training loss: 1.86328125 \t 22:48:52.844938\n",
      "Step  133 \t training loss: 1.9072265625 \t 22:49:03.597479\n",
      "Step  134 \t training loss: 1.75 \t 22:49:14.353945\n",
      "Step  135 \t training loss: 1.751953125 \t 22:49:25.121690\n",
      "Step  136 \t training loss: 1.82421875 \t 22:49:35.985258\n",
      "Step  137 \t training loss: 1.8388671875 \t 22:49:46.814247\n",
      "Step  138 \t training loss: 1.751953125 \t 22:49:57.594844\n",
      "Step  139 \t training loss: 1.8486328125 \t 22:50:08.498981\n",
      "Step  140 \t training loss: 1.85546875 \t 22:50:19.695735\n",
      "validation loss: 1.830078125\n",
      "Epoch     8: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Step  141 \t training loss: 1.8564453125 \t 22:50:31.082961\n",
      "Step  142 \t training loss: 1.9384765625 \t 22:50:42.193675\n",
      "Step  143 \t training loss: 1.6572265625 \t 22:50:53.712793\n",
      "Step  144 \t training loss: 2.01171875 \t 22:51:05.250924\n",
      "Step  145 \t training loss: 1.900390625 \t 22:51:16.718993\n",
      "Step  146 \t training loss: 1.7744140625 \t 22:51:27.847013\n",
      "Step  147 \t training loss: 1.89453125 \t 22:51:38.879978\n",
      "Step  148 \t training loss: 1.798828125 \t 22:51:49.907636\n",
      "Step  149 \t training loss: 1.734375 \t 22:52:00.944078\n",
      "Step  150 \t training loss: 1.88671875 \t 22:52:11.982535\n",
      "Step  151 \t training loss: 1.6123046875 \t 22:52:23.018864\n",
      "Step  152 \t training loss: 1.712890625 \t 22:52:34.062759\n",
      "Step  153 \t training loss: 1.9345703125 \t 22:52:45.081645\n",
      "Step  154 \t training loss: 1.8515625 \t 22:52:55.894308\n",
      "Step  155 \t training loss: 1.740234375 \t 22:53:06.728281\n",
      "Step  156 \t training loss: 1.7880859375 \t 22:53:17.561485\n",
      "Step  157 \t training loss: 1.73828125 \t 22:53:28.355766\n",
      "Step  158 \t training loss: 1.6982421875 \t 22:53:39.161722\n",
      "Step  159 \t training loss: 1.8447265625 \t 22:53:49.956403\n",
      "Step  160 \t training loss: 1.630859375 \t 22:54:00.764108\n",
      "validation loss: 1.8291015625\n",
      "Step  161 \t training loss: 1.7275390625 \t 22:54:11.940692\n",
      "Step  162 \t training loss: 1.8671875 \t 22:54:22.761221\n",
      "Step  163 \t training loss: 1.708984375 \t 22:54:33.575310\n"
     ]
    }
   ],
   "source": [
    "# for i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n",
    "i = 0\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "while True:\n",
    "    \n",
    "# for batch_idx, batch in enumerate(tqdm(train_loader, mininterval=2, leave=False)):\n",
    "    batch_qs, batch_qs_mask, batch_as, batch_as_mask = map(lambda x: x.to(device), next(train_loader))\n",
    "    # exclude the 0th element as it is BOS\n",
    "    gold_as = batch_as[:, 1:]\n",
    "    \n",
    "    if (i % GENERATE_EVERY) - 1 == 0:\n",
    "        enc_dec.eval()\n",
    "        gen_qs, gen_qs_mask, gen_as, gen_as_mask = next(gen_loader)\n",
    "    #         inp = random.choice(val_ds)[:-1]\n",
    "        prime = np_decode_string(gen_qs.numpy())\n",
    "        print('*' * 100, \"\\nQuestion: \", prime)\n",
    "        print(\"Actual Answer: \", np_decode_string(gen_as.numpy()))\n",
    "    #     print(\"Raw Answer: \", gen_as.numpy())\n",
    "        gen_qs = gen_qs.to(device)\n",
    "        gen_as = gen_as.to(device)\n",
    "        gen_qs_mask = gen_qs_mask.to(device)\n",
    "        sample = enc_dec.generate(gen_qs, gen_as, GENERATE_LENGTH, enc_input_mask = gen_qs_mask)\n",
    "        sample = sample.cpu().numpy()\n",
    "        output_str = np_decode_string(sample)\n",
    "    #     print(\"Raw Prediction: \", sample)\n",
    "        print(\"Decoded Prediction: \", output_str)\n",
    "        with open(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-train_loss.txt\", \"wb\") as fp:\n",
    "            pickle.dump(train_loss_list, fp)\n",
    "        with open(base_dir + \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\", \"wb\") as fp:\n",
    "            pickle.dump(val_loss_list, fp)\n",
    "        print(\"Logs saved to \", \"logs/\" + exp_name + \"_\" + unique_id + \"-val_loss.txt\")\n",
    "            \n",
    "            \n",
    "\n",
    "    enc_dec.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        train_loss = enc_dec(batch_qs, batch_as, return_loss = True, enc_input_mask = batch_qs_mask)\n",
    "        with amp.scale_loss(train_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    \n",
    "#     if batch_idx % GRADIENT_ACCUMULATE_EVERY == 0:\n",
    "    print(\"Step \", i, \"\\t\", f'training loss: {train_loss.item()}', \"\\t\", datetime.now().time() )\n",
    "    train_loss_list.append((i, train_loss.item()))\n",
    "    torch.nn.utils.clip_grad_norm_(enc_dec.parameters(), 0.1)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        val_batch_qs, val_batch_qs_mask, val_batch_as, val_batch_as_mask = map(lambda x: x.to(device), next(val_loader))\n",
    "        enc_dec.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = enc_dec(val_batch_qs, val_batch_as, return_loss = True, enc_input_mask = val_batch_qs_mask)\n",
    "            print(f'validation loss: {val_loss.item()}')\n",
    "            val_loss_list.append((i, val_loss.item()))\n",
    "            scheduler.step(val_loss)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([train_loss_list[1]])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
